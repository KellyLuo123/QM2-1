[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods 2",
    "section": "",
    "text": "This course teaches quantitative skills, with an emphasis on the context and use of data. Students learn to focus on datasets which will allow them to explore questions in society – in arts, humanities, sports, criminal justice, economics, inequality, or policy. Students are expected to work with Python to carry out data manipulation (cleaning and segmentation), analysis (for example, deriving descriptive statistics) and visualisation (graphing, mapping and other forms of visualisation). They will engage with literatures around a topic and connect their datasets and analyses to explore and decide wider arguments, and link their results to these contextual considerations."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Week 1.html#week-1-recapping-python",
    "href": "Week 1.html#week-1-recapping-python",
    "title": "2  Week 1",
    "section": "2.2 Week 1: Recapping Python",
    "text": "2.2 Week 1: Recapping Python"
  },
  {
    "objectID": "Week 1.html#using-python",
    "href": "Week 1.html#using-python",
    "title": "2  Week 1",
    "section": "2.3 Using Python",
    "text": "2.3 Using Python\nIn this course, we’ll make extensive use of Python, a programming language used widely in scientific computing and on the web. We will be using Python as a way to manipulate, plot and analyse data. This isn’t a course about learning Python, it’s about working with data - but we’ll learning a little bit of programming along the way.\nBy now, you should have done the prerequisites for the module, and understand a bit about how Python is structured, what different commands do, and so on - this is a bit of a refresher to remind you of what we need at the beginning of term.\nThe particular flavour of Python we’re using is iPython, which, as we’ve seen, allows us to combine text, code, images, equations and figures in a Notebook. This is a cell, written in markdown - a way of writing nice text. Contrast this with code cell, which executes a bit of Python: print(1+1) The Notebook format allows you to engage in what Don Knuth describes as Literate Programming:\n\n[…] Instead of writing code containing documentation, the literate programmer writes documentation containing code. No longer does the English commentary injected into a program have to be hidden in comment delimiters at the top of the file, or under procedure headings, or at the end of lines. Instead, it is wrenched into the daylight and made the main focus. The “program” then becomes primarily a document directed at humans, with the code being herded between “code delimiters” from where it can be extracted and shuffled out sideways to the language system by literate programming tools. Ross Williams"
  },
  {
    "objectID": "Week 1.html#libraries",
    "href": "Week 1.html#libraries",
    "title": "2  Week 1",
    "section": "2.4 Libraries",
    "text": "2.4 Libraries\nWe will work with a number of libraries, which provide additional functions and techniques to help us to carry out our tasks.\nThese include:\nPandas: we’ll use this a lot to slice and dice data\nmatplotlib: this is our basic graphing software, and we’ll also use it for mapping\nnltk: The Natural Language Tool Kit will help us work with text\nWe aren’t doing all this to learn to program. We could spend a whole term learning how to use Python and never look at any data, maps, graphs, or visualisations. But we do need to understand a few basics to use Python for working with data. So let’s revisit a few concepts that you should have covered in your prerequisites. Variables ———\nPython can broadly be divided in verbs and nouns: things which do things, and things which are things. In Python, the verbs can be commands, functions, or methods. We won’t worry too much about the distinction here - suffice it to say, they are the parts of code which manipulate data, calculate values, or show things on the screen.\nThe simplest proper noun object in Python is the variable. Variables are given names and store information. This can be, for example, numeric, text, or boolean (true/false). These are all statements setting up variables:\nn = 1\nt = “hi”\nb = True\nNow let’s try this in code: n = 1\nt = “hi”\nb = True Note that each command is on a new line; other than that, the syntax of Python should be fairly clear. We’re setting these variables equal to the letters and numbers and phrases and booleans. What’s a boolean?\nThe value of this is we now have values tied to these variables - so every time we want to use it, we can refer to the variable: n t b Because we’ve defined these variables in the early part of the notebook, we can use them later on. Advanced: where do classes fit into this noun/verb picture of variables and commands? Where is my data? —————–\nWhen we work in excel and text editors, we’re used to seeing the data onscreen - and if we manipulate the data in some way (averaging or summing up), we see both the inputs and outputs on screen. The big difference in working with Python is that we don’t see our variables all of the time, or the effect we’re having on them. They’re there in the background, but it’s usually worth checking in on them from time to time, to see whether our processes are doing what we think they’re doing.\nThis is pretty easy to do - we can just type the variable name, or “print(variable name)”: n = n+1 print(n) print(t) print(b) Flow —-\nPython, in common with all programming languages, executes commands in a sequence - we might refer to this as the “ineluctable march of the machines”, but it’s more common referred to as the flow of the code (we’ll use the word “code” a lot - it just means commands written in the programming language). In most cases, code just executes in the order it’s written. This is true within each cell (each block of text in the notebook), and it’s true when we execute the cells in order; that’s why we can refer back to the variables we defined earlier: print(n) If we make a change to one of these variables, say n: n = 3 and execute the above “print n” command, you’ll see that it has changed n to 3. So if we go out of order, the obvious flow of the code is confused. For this reason, try to write your code so it executes in order, one cell at a time. At least for the moment, this will make it easier to follow the logic of what you’re doing to data. Advanced: what happens to this flow when you write functions to automate common tasks? Exercise - Setting up variables:\n\nCreate a new cell.\nCreate the variables “name”, and assign your name to it.\nCreate a variable “Python” and assign a score out of 10 to how much you like Python.\nCreate a variable “prior” and if you’ve used Python before, assign True; otherwise assign False to the variable\nPrint these out to the screen"
  },
  {
    "objectID": "Week 1.html#downloading-data",
    "href": "Week 1.html#downloading-data",
    "title": "2  Week 1",
    "section": "2.5 Downloading Data",
    "text": "2.5 Downloading Data\nLets fetch the data we will be using for this session. There are two ways in which you can upload data to the Colab notebook. You can use the following code to upload a CSV or similar data file.\nfrom google.colab import files uploaded = files.upload() Or you can use the following cell to fetch the data directly from the QM2 server.\nLet’s create a folder that we can store all our data for this session !mkdir data !mkdir ./data/wk1 !curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/data.csv -o ./data/wk1/data.csv !curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/sample_group.csv -o ./data/wk1/sample_group.csv Storing and importing data ————————–\nTypically, data we look at won’t be just one number, or one bit of text. Python has a lot of different ways of dealing with a bunch of numbers: for example, a list of values is called a list: listy = [1,2,3,6,9] print(listy) A set of values linked to an index (or key) is called a dictionary; for example: dicty = {‘Bob’: 1.2, ‘Mike’: 1.2, ‘Coop’: 1.1, ‘Maddy’: 1.3, ‘Giant’: 2.1} print(dicty) Notice that the list uses square brackets with values separated by commas, and the dict uses curly brackets with pairs separated by commas, and colons (:) to link a key (index or address) with a value.\n(You might notice that they haven’t printed out in the order you entered them) *Advanced: Print out 1) The third element of listy, and 2) The element of dicty relating to Giant\nWe’ll discuss different ways of organising data again soon, but for now we’ll look at dataframes - the way our data-friendly library Pandas works with data. We’ll be using Pandas a lot this term, so it’s good to get started with it early.\nLet’s start by importing pandas. We’ll also import another library, but we’re not going to worry about that too much at the moment.\nIf you see a warning about ‘Building Font Cache’ don’t worry - this is normal. import pandas\nimport matplotlib %matplotlib inline Let’s import a simple dataset and show it in pandas. We’ll use a pre-prepared “.csv” file, which needs to be in the same folder as our code. data = pandas.read_csv(‘./data/wk1/data.csv’) data.head()\nWhat we’ve done here is read in a .csv file into a dataframe, the object pandas uses to work with data, and one that has lots of methods for slicing and dicing data, as we will see over the coming weeks. The head() command tells iPython to show the first few columns/rows of the data, so we can start to get a sense of what the data looks like and what sort of type of objects is represents. # Supplementary: Kaggle exercises\nIf you’ve gotten this far, congratulations! To further hone your skills, try working your way through the five intro to programming notebooks on Kaggle. These cover a range of skills that we’ll be using throughout the term. Kaggle is a very useful resource for learning data science, so making an account may not be a bad idea!"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#using-python",
    "href": "notebooks/W1. Python Recap.html#using-python",
    "title": "Week 1: Recapping Python",
    "section": "Using Python",
    "text": "Using Python\nIn this course, we’ll make extensive use of Python, a programming language used widely in scientific computing and on the web. We will be using Python as a way to manipulate, plot and analyse data. This isn’t a course about learning Python, it’s about working with data - but we’ll learning a little bit of programming along the way.\nBy now, you should have done the prerequisites for the module, and understand a bit about how Python is structured, what different commands do, and so on - this is a bit of a refresher to remind you of what we need at the beginning of term.\nThe particular flavour of Python we’re using is iPython, which, as we’ve seen, allows us to combine text, code, images, equations and figures in a Notebook. This is a cell, written in markdown - a way of writing nice text. Contrast this with code cell, which executes a bit of Python:\n\nprint(1+1)\n\nThe Notebook format allows you to engage in what Don Knuth describes as Literate Programming:\n\n[…] Instead of writing code containing documentation, the literate programmer writes documentation containing code. No longer does the English commentary injected into a program have to be hidden in comment delimiters at the top of the file, or under procedure headings, or at the end of lines. Instead, it is wrenched into the daylight and made the main focus. The “program” then becomes primarily a document directed at humans, with the code being herded between “code delimiters” from where it can be extracted and shuffled out sideways to the language system by literate programming tools. Ross Williams"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#libraries",
    "href": "notebooks/W1. Python Recap.html#libraries",
    "title": "Week 1: Recapping Python",
    "section": "Libraries",
    "text": "Libraries\nWe will work with a number of libraries, which provide additional functions and techniques to help us to carry out our tasks.\nThese include:\nPandas: we’ll use this a lot to slice and dice data\nmatplotlib: this is our basic graphing software, and we’ll also use it for mapping\nnltk: The Natural Language Tool Kit will help us work with text\nWe aren’t doing all this to learn to program. We could spend a whole term learning how to use Python and never look at any data, maps, graphs, or visualisations. But we do need to understand a few basics to use Python for working with data. So let’s revisit a few concepts that you should have covered in your prerequisites."
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#variables",
    "href": "notebooks/W1. Python Recap.html#variables",
    "title": "Week 1: Recapping Python",
    "section": "Variables",
    "text": "Variables\nPython can broadly be divided in verbs and nouns: things which do things, and things which are things. In Python, the verbs can be commands, functions, or methods. We won’t worry too much about the distinction here - suffice it to say, they are the parts of code which manipulate data, calculate values, or show things on the screen.\nThe simplest proper noun object in Python is the variable. Variables are given names and store information. This can be, for example, numeric, text, or boolean (true/false). These are all statements setting up variables:\nn = 1\nt = “hi”\nb = True\nNow let’s try this in code:\n\nn = 1\n\nt = \"hi\"\n\nb = True\n\nNote that each command is on a new line; other than that, the syntax of Python should be fairly clear. We’re setting these variables equal to the letters and numbers and phrases and booleans. What’s a boolean?\nThe value of this is we now have values tied to these variables - so every time we want to use it, we can refer to the variable:\n\nn\n\n\nt\n\n\nb\n\nBecause we’ve defined these variables in the early part of the notebook, we can use them later on.\nAdvanced: where do classes fit into this noun/verb picture of variables and commands?"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#where-is-my-data",
    "href": "notebooks/W1. Python Recap.html#where-is-my-data",
    "title": "Week 1: Recapping Python",
    "section": "Where is my data?",
    "text": "Where is my data?\nWhen we work in excel and text editors, we’re used to seeing the data onscreen - and if we manipulate the data in some way (averaging or summing up), we see both the inputs and outputs on screen. The big difference in working with Python is that we don’t see our variables all of the time, or the effect we’re having on them. They’re there in the background, but it’s usually worth checking in on them from time to time, to see whether our processes are doing what we think they’re doing.\nThis is pretty easy to do - we can just type the variable name, or “print(variable name)”:\n\nn = n+1\nprint(n)\nprint(t)\nprint(b)"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#flow",
    "href": "notebooks/W1. Python Recap.html#flow",
    "title": "Week 1: Recapping Python",
    "section": "Flow",
    "text": "Flow\nPython, in common with all programming languages, executes commands in a sequence - we might refer to this as the “ineluctable march of the machines”, but it’s more common referred to as the flow of the code (we’ll use the word “code” a lot - it just means commands written in the programming language). In most cases, code just executes in the order it’s written. This is true within each cell (each block of text in the notebook), and it’s true when we execute the cells in order; that’s why we can refer back to the variables we defined earlier:\n\nprint(n)\n\nIf we make a change to one of these variables, say n:\n\nn = 3\n\nand execute the above “print n” command, you’ll see that it has changed n to 3. So if we go out of order, the obvious flow of the code is confused. For this reason, try to write your code so it executes in order, one cell at a time. At least for the moment, this will make it easier to follow the logic of what you’re doing to data.\nAdvanced: what happens to this flow when you write functions to automate common tasks?\nExercise - Setting up variables:\n\nCreate a new cell.\nCreate the variables “name”, and assign your name to it.\nCreate a variable “Python” and assign a score out of 10 to how much you like Python.\nCreate a variable “prior” and if you’ve used Python before, assign True; otherwise assign False to the variable\nPrint these out to the screen"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#downloading-data",
    "href": "notebooks/W1. Python Recap.html#downloading-data",
    "title": "Week 1: Recapping Python",
    "section": "Downloading Data",
    "text": "Downloading Data\nLets fetch the data we will be using for this session. There are two ways in which you can upload data to the Colab notebook. You can use the following code to upload a CSV or similar data file.\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nKeyboardInterrupt: ignored\n\n\nOr you can use the following cell to fetch the data directly from the QM2 server.\nLet’s create a folder that we can store all our data for this session\n\n!mkdir data\n\n\n!mkdir ./data/wk1\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/data.csv -o ./data/wk1/data.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/sample_group.csv -o ./data/wk1/sample_group.csv\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   203  100   203    0     0    527      0 --:--:-- --:--:-- --:--:--   527\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   297  100   297    0     0    838      0 --:--:-- --:--:-- --:--:--   836"
  },
  {
    "objectID": "notebooks/W1. Python Recap.html#storing-and-importing-data",
    "href": "notebooks/W1. Python Recap.html#storing-and-importing-data",
    "title": "Week 1: Recapping Python",
    "section": "Storing and importing data",
    "text": "Storing and importing data\nTypically, data we look at won’t be just one number, or one bit of text. Python has a lot of different ways of dealing with a bunch of numbers: for example, a list of values is called a list:\n\nlisty = [1,2,3,6,9]\nprint(listy)\n\n[1, 2, 3, 6, 9]\n\n\nA set of values linked to an index (or key) is called a dictionary; for example:\n\ndicty = {'Bob': 1.2, 'Mike': 1.2, 'Coop': 1.1, 'Maddy': 1.3, 'Giant': 2.1}\nprint(dicty)\n\n{'Bob': 1.2, 'Mike': 1.2, 'Coop': 1.1, 'Maddy': 1.3, 'Giant': 2.1}\n\n\nNotice that the list uses square brackets with values separated by commas, and the dict uses curly brackets with pairs separated by commas, and colons (:) to link a key (index or address) with a value.\n(You might notice that they haven’t printed out in the order you entered them)\n*Advanced: Print out 1) The third element of listy, and 2) The element of dicty relating to Giant\nWe’ll discuss different ways of organising data again soon, but for now we’ll look at dataframes - the way our data-friendly library Pandas works with data. We’ll be using Pandas a lot this term, so it’s good to get started with it early.\nLet’s start by importing pandas. We’ll also import another library, but we’re not going to worry about that too much at the moment.\nIf you see a warning about ‘Building Font Cache’ don’t worry - this is normal.\n\nimport pandas\n\nimport matplotlib\n%matplotlib inline\n\nLet’s import a simple dataset and show it in pandas. We’ll use a pre-prepared “.csv” file, which needs to be in the same folder as our code.\n\ndata = pandas.read_csv('./data/wk1/data.csv')\ndata.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      First Appearance\n      Approx height\n      Gender\n      Law Enforcement\n    \n  \n  \n    \n      0\n      Bob\n      1.2\n      6.0\n      Male\n      False\n    \n    \n      1\n      Mike\n      1.2\n      5.5\n      Male\n      False\n    \n    \n      2\n      Coop\n      1.1\n      6.0\n      Male\n      True\n    \n    \n      3\n      Maddy\n      1.3\n      5.5\n      Female\n      False\n    \n    \n      4\n      Giant\n      2.1\n      7.5\n      Male\n      False\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhat we’ve done here is read in a .csv file into a dataframe, the object pandas uses to work with data, and one that has lots of methods for slicing and dicing data, as we will see over the coming weeks. The head() command tells iPython to show the first few columns/rows of the data, so we can start to get a sense of what the data looks like and what sort of type of objects is represents."
  },
  {
    "objectID": "Week 1.html",
    "href": "Week 1.html",
    "title": "2  Week 1",
    "section": "",
    "text": "This is an embedded <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> presentation, powered by <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "notebooks/W1. Python Recap.html",
    "href": "notebooks/W1. Python Recap.html",
    "title": "Week 1: Recapping Python",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html",
    "href": "notebooks/W01. Python Recap.html",
    "title": "Python Recap",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#using-python",
    "href": "notebooks/W01. Python Recap.html#using-python",
    "title": "Python Recap",
    "section": "Using Python",
    "text": "Using Python\nIn this course, we’ll make extensive use of Python, a programming language used widely in scientific computing and on the web. We will be using Python as a way to manipulate, plot and analyse data. This isn’t a course about learning Python, it’s about working with data - but we’ll learning a little bit of programming along the way.\nBy now, you should have done the prerequisites for the module, and understand a bit about how Python is structured, what different commands do, and so on - this is a bit of a refresher to remind you of what we need at the beginning of term.\nThe particular flavour of Python we’re using is iPython, which, as we’ve seen, allows us to combine text, code, images, equations and figures in a Notebook. This is a cell, written in markdown - a way of writing nice text. Contrast this with code cell, which executes a bit of Python:\n\nprint(1+1)\n\nThe Notebook format allows you to engage in what Don Knuth describes as Literate Programming:\n\n[…] Instead of writing code containing documentation, the literate programmer writes documentation containing code. No longer does the English commentary injected into a program have to be hidden in comment delimiters at the top of the file, or under procedure headings, or at the end of lines. Instead, it is wrenched into the daylight and made the main focus. The “program” then becomes primarily a document directed at humans, with the code being herded between “code delimiters” from where it can be extracted and shuffled out sideways to the language system by literate programming tools. Ross Williams"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#libraries",
    "href": "notebooks/W01. Python Recap.html#libraries",
    "title": "Python Recap",
    "section": "Libraries",
    "text": "Libraries\nWe will work with a number of libraries, which provide additional functions and techniques to help us to carry out our tasks.\nThese include:\nPandas: we’ll use this a lot to slice and dice data\nmatplotlib: this is our basic graphing software, and we’ll also use it for mapping\nnltk: The Natural Language Tool Kit will help us work with text\nWe aren’t doing all this to learn to program. We could spend a whole term learning how to use Python and never look at any data, maps, graphs, or visualisations. But we do need to understand a few basics to use Python for working with data. So let’s revisit a few concepts that you should have covered in your prerequisites."
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#variables",
    "href": "notebooks/W01. Python Recap.html#variables",
    "title": "Python Recap",
    "section": "Variables",
    "text": "Variables\nPython can broadly be divided in verbs and nouns: things which do things, and things which are things. In Python, the verbs can be commands, functions, or methods. We won’t worry too much about the distinction here - suffice it to say, they are the parts of code which manipulate data, calculate values, or show things on the screen.\nThe simplest proper noun object in Python is the variable. Variables are given names and store information. This can be, for example, numeric, text, or boolean (true/false). These are all statements setting up variables:\nn = 1\nt = “hi”\nb = True\nNow let’s try this in code:\n\nn = 1\n\nt = \"hi\"\n\nb = True\n\nNote that each command is on a new line; other than that, the syntax of Python should be fairly clear. We’re setting these variables equal to the letters and numbers and phrases and booleans. What’s a boolean?\nThe value of this is we now have values tied to these variables - so every time we want to use it, we can refer to the variable:\n\nn\n\n\nt\n\n\nb\n\nBecause we’ve defined these variables in the early part of the notebook, we can use them later on.\nAdvanced: where do classes fit into this noun/verb picture of variables and commands?"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#where-is-my-data",
    "href": "notebooks/W01. Python Recap.html#where-is-my-data",
    "title": "Python Recap",
    "section": "Where is my data?",
    "text": "Where is my data?\nWhen we work in excel and text editors, we’re used to seeing the data onscreen - and if we manipulate the data in some way (averaging or summing up), we see both the inputs and outputs on screen. The big difference in working with Python is that we don’t see our variables all of the time, or the effect we’re having on them. They’re there in the background, but it’s usually worth checking in on them from time to time, to see whether our processes are doing what we think they’re doing.\nThis is pretty easy to do - we can just type the variable name, or “print(variable name)”:\n\nn = n+1\nprint(n)\nprint(t)\nprint(b)"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#flow",
    "href": "notebooks/W01. Python Recap.html#flow",
    "title": "Python Recap",
    "section": "Flow",
    "text": "Flow\nPython, in common with all programming languages, executes commands in a sequence - we might refer to this as the “ineluctable march of the machines”, but it’s more common referred to as the flow of the code (we’ll use the word “code” a lot - it just means commands written in the programming language). In most cases, code just executes in the order it’s written. This is true within each cell (each block of text in the notebook), and it’s true when we execute the cells in order; that’s why we can refer back to the variables we defined earlier:\n\nprint(n)\n\nIf we make a change to one of these variables, say n:\n\nn = 3\n\nand execute the above “print n” command, you’ll see that it has changed n to 3. So if we go out of order, the obvious flow of the code is confused. For this reason, try to write your code so it executes in order, one cell at a time. At least for the moment, this will make it easier to follow the logic of what you’re doing to data.\nAdvanced: what happens to this flow when you write functions to automate common tasks?\nExercise - Setting up variables:\n\nCreate a new cell.\nCreate the variables “name”, and assign your name to it.\nCreate a variable “Python” and assign a score out of 10 to how much you like Python.\nCreate a variable “prior” and if you’ve used Python before, assign True; otherwise assign False to the variable\nPrint these out to the screen"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#downloading-data",
    "href": "notebooks/W01. Python Recap.html#downloading-data",
    "title": "Python Recap",
    "section": "Downloading Data",
    "text": "Downloading Data\nLets fetch the data we will be using for this session. There are two ways in which you can upload data to the Colab notebook. You can use the following code to upload a CSV or similar data file.\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nKeyboardInterrupt: ignored\n\n\nOr you can use the following cell to fetch the data directly from the QM2 server.\nLet’s create a folder that we can store all our data for this session\n\n!mkdir data\n\n\n!mkdir ./data/wk1\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/data.csv -o ./data/wk1/data.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk1/sample_group.csv -o ./data/wk1/sample_group.csv\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   203  100   203    0     0    527      0 --:--:-- --:--:-- --:--:--   527\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   297  100   297    0     0    838      0 --:--:-- --:--:-- --:--:--   836"
  },
  {
    "objectID": "notebooks/W01. Python Recap.html#storing-and-importing-data",
    "href": "notebooks/W01. Python Recap.html#storing-and-importing-data",
    "title": "Python Recap",
    "section": "Storing and importing data",
    "text": "Storing and importing data\nTypically, data we look at won’t be just one number, or one bit of text. Python has a lot of different ways of dealing with a bunch of numbers: for example, a list of values is called a list:\n\nlisty = [1,2,3,6,9]\nprint(listy)\n\n[1, 2, 3, 6, 9]\n\n\nA set of values linked to an index (or key) is called a dictionary; for example:\n\ndicty = {'Bob': 1.2, 'Mike': 1.2, 'Coop': 1.1, 'Maddy': 1.3, 'Giant': 2.1}\nprint(dicty)\n\n{'Bob': 1.2, 'Mike': 1.2, 'Coop': 1.1, 'Maddy': 1.3, 'Giant': 2.1}\n\n\nNotice that the list uses square brackets with values separated by commas, and the dict uses curly brackets with pairs separated by commas, and colons (:) to link a key (index or address) with a value.\n(You might notice that they haven’t printed out in the order you entered them)\n*Advanced: Print out 1) The third element of listy, and 2) The element of dicty relating to Giant\nWe’ll discuss different ways of organising data again soon, but for now we’ll look at dataframes - the way our data-friendly library Pandas works with data. We’ll be using Pandas a lot this term, so it’s good to get started with it early.\nLet’s start by importing pandas. We’ll also import another library, but we’re not going to worry about that too much at the moment.\nIf you see a warning about ‘Building Font Cache’ don’t worry - this is normal.\n\nimport pandas\n\nimport matplotlib\n%matplotlib inline\n\nLet’s import a simple dataset and show it in pandas. We’ll use a pre-prepared “.csv” file, which needs to be in the same folder as our code.\n\ndata = pandas.read_csv('./data/wk1/data.csv')\ndata.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      First Appearance\n      Approx height\n      Gender\n      Law Enforcement\n    \n  \n  \n    \n      0\n      Bob\n      1.2\n      6.0\n      Male\n      False\n    \n    \n      1\n      Mike\n      1.2\n      5.5\n      Male\n      False\n    \n    \n      2\n      Coop\n      1.1\n      6.0\n      Male\n      True\n    \n    \n      3\n      Maddy\n      1.3\n      5.5\n      Female\n      False\n    \n    \n      4\n      Giant\n      2.1\n      7.5\n      Male\n      False\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhat we’ve done here is read in a .csv file into a dataframe, the object pandas uses to work with data, and one that has lots of methods for slicing and dicing data, as we will see over the coming weeks. The head() command tells iPython to show the first few columns/rows of the data, so we can start to get a sense of what the data looks like and what sort of type of objects is represents."
  },
  {
    "objectID": "notebooks/W02. Pandas.html",
    "href": "notebooks/W02. Pandas.html",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "",
    "text": "Open In Colab\nIn this workshop, our aim is to get used to working with more complex data that we’ve imported from external files. We’ll start to graph it, and to slice and dice it, to select the bits we’re interested in.\nWe will work with pandas to manipulate the data, and to derive measures and graphs that tell us a bit more than what the source data files tell us."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#introduction",
    "href": "notebooks/W02. Pandas.html#introduction",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Introduction",
    "text": "Introduction\nWe are going to work with some UK income data. The income data is packaged as a .csv file. The Pandas package knows how to handle this and put the data in a DataFrame, as we’ve seen. Let’s examine the data and start to see what we can say about it. First of all, we have to find data - I’m interested in looking in data with a wide spread, so I looked for data on income in the UK.\nThis data is collected by the Office for National Statistics(ONS) : http://www.ons.gov.uk/ons/datasets-and-tables/index.html?pageSize=50&sortBy=none&sortDirection=none&newquery=income+percentile - but the exact data I want to see, income by percentile, is tricky to find.\nI ended up using data from 2011, generated from a study called the Family Resources Survey and collated and tweaked by an independent research unit called the Institute of Fiscal Studies (IFS). The “tweaking” they do tends to be around the size of the family unit, and other factors which create economies of scale - hence they “equivalise” it. The IFS is quoted in UK Government documents, so we can have some trust in their impartiality, or at least accuracy - of course, if we were publishing research about this, that’s not really good enough and we’d want to reproduce, or at least understand and critique, their methodology rather than just trusting it!\ne.g.:\nhttp://www.ifs.org.uk/wheredoyoufitin/about.php\nhttps://en.wikipedia.org/wiki/Equivalisation"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#downloading-the-data",
    "href": "notebooks/W02. Pandas.html#downloading-the-data",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Downloading the Data",
    "text": "Downloading the Data\nLet’s grab our income data from our course website and save it into our data folder. If you’ve not already created a data folder then do so using the following command. Don’t worry if it generates an error, that means you’ve already got a data folder.\n\n!mkdir data\n\n\n!mkdir data/wk2\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk2/incomes.csv -o ./data/wk2/incomes.csv\n\n\nimport pandas\nimport pylab\nimport matplotlib.pyplot as plt\n# make the plots a little wider by default\n%matplotlib inline\nplt.style.use('ggplot')\n\npylab.rcParams['figure.figsize'] = (10., 8.)\n\n\ndata_path = \"./data/wk2/incomes.csv\"\n\nincome =  pandas.read_csv(data_path, index_col=0)\nincome.head()\n\nThis is a simple dataframe - we see the percentile and an income. Note that I’ve told pandas to use the first column (the Percentile) as the index to make life easier.\nThe percentile tells us how people on that income rank - so the final category, 99% (which is really binned, so 99%<n\\(\\leq\\) 100%), is telling us how much “the 1%” earn. Let’s find out:\n\nincome.tail()\n\nWell, they we have it - the 1% earn, on average, about £2000 a week. How does that compare to people in the 90% decile? We can access particular rows in a dataframe using .loc[row index]; because our index is the percentile point, we can just read it off:\n\nincome.loc[90]\n\nWe can also select a range of values with the “colon” notation. This will select the 90-95th percentiles, for example:\n\nincome.loc[90:95]"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#accessing-parts-of-a-dataframe",
    "href": "notebooks/W02. Pandas.html#accessing-parts-of-a-dataframe",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Accessing parts of a dataframe",
    "text": "Accessing parts of a dataframe\nIf we want to extract the actual value instead of just the whole row, we need to reference the column as well as the row. In pandas, columns are referenced by column name:\n\nincome['Net equivalised household income in 2010-11, week']\n\nSo, to access a particular cell, we tell Python the row and the column (this is pretty simple - the same way we tell excel to access cell “A34” meaning Column A, Row 34). One way we do that in pandas is to select the column, and then use .loc[] on the index.\n\nincome['Net equivalised household income in 2010-11, week'].loc[90]\n\nWe’ve accessed row 90 of the column called ‘Net equivalised household income in 2010-11, week’; can we access the data the other way around - can we first take the row and then specify a column? Let’s try:\n\nincome.loc[90]['Net equivalised household income in 2010-11, week']\n\nYes, this seems to be working fine.\n\nExtension\nThe reason for this is that selecting the column spits out a smaller dataframe, and all dataframes use “loc”, so we can use that. Another way to do this would be to use an explicit variable for the dataframe, along the lines of:\nsmallDataFrame = income['Net equivalised household income in 2010-11, week']\nsmallDataFrame.loc[90]\nby doing income\n['Net equivalised household income in 2010-11, week'].loc[90]\nwe’re taking the “smallDataFrame” object as an implicit (or hidden) output\nIf we want to look at a few rows of data, we can use a range:\n\nincome['Net equivalised household income in 2010-11, week'].loc[90:95]\n\nSo, to recap, we can now access a particular row using loc[index number], a particular column with the square brackets formalism dataframename[‘column name’], or both dataframename[‘column name’].loc[index number]. We’ve made a start at being able to get to the bits of data we need."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#exercise",
    "href": "notebooks/W02. Pandas.html#exercise",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Exercise:",
    "text": "Exercise:\nHow do the equivalised incomes of single adults and childless couples compare? Look at the 1st, 99th and 50th percentile and summarise what this tells you about the value or price of coupling."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#examining-the-distribution",
    "href": "notebooks/W02. Pandas.html#examining-the-distribution",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Examining the Distribution",
    "text": "Examining the Distribution\nReturning to the overall statistics, the 90% percentile earns less than half the top percentile (“the 1%”); if you’re taking home over £800 as a household, you’re in the top 10% of earners.\nHow does 1. The income of “the 1%” compare with the mean and median across the population, as a proportion? 2. How does the 1% compare with the 90th percentile (the 10%)? 3. How does the 10% compare with the median and mean?\nThe 1% earn about 60 times the poorest groups in society - and we’ve made other comparisons. But that’s not the whole story. Let’s look at the income graph.\nIn pandas, we can plot this fairly easily…\n\nincome['Net equivalised household income in 2010-11, week'].plot()\nplt.title('UK Net Equivalised Income by Percentile per week, 2010-11')\nplt.xlabel('Income Percentile')\nplt.ylabel('Income (Net, Equivalised) [GBP]')\n\nWe see a curve that is pretty linear in the middle region, but curves rapidly upwards in the higher percentile and looks more like a power law.\n\nExercise: Means\nWhere does the mean appear here? Draw in a horizontal line to show the mean using axhline. Show the median on the same graph. What is the meaning of the median in this context?\nHint: Recall that last time we used axvline to highlight the mean and standard deviation by drawing vertical lines on the axis. Here, we use axhline to draw horizontal lines.\n\n\nExtension: Accessing cells\nThere are a number of ways to access elements of the dataframe: we’ve shown how to access columns by the [‘name of column’] method, and rows via the .loc[index] method; and how we can select a range. There are also .iloc methods to select by number rather than name; you should become familiar with these on the documentation page for pandas."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#comparing-segments",
    "href": "notebooks/W02. Pandas.html#comparing-segments",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Comparing segments",
    "text": "Comparing segments\nEarlier, we compared some summary statistics of single people and couples. Let’s look at the wider curve for more than one group, now:\n\n#This is going to throw a load of errors\nincome[['Single adult','Lone parent, one child under 14']].plot()"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#warning",
    "href": "notebooks/W02. Pandas.html#warning",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Warning",
    "text": "Warning\nThis isn’t looking good. There’s a load of text and no graph. If you’ve not seen this before, it’s an error - something has gone wrong. Generally, if we look at the final line, it should tell us what’s wrong, in this case there’s “no numeric data to plot”, which is weird, because we’ve seen the data and have even plotted some of it."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#messy-data",
    "href": "notebooks/W02. Pandas.html#messy-data",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Messy Data",
    "text": "Messy Data\nDataFrames, as we are starting to see, give us the chance to plot, chop, slice and data to help us make sense of it. Here, we will create a new DataFrame to take only two columns of data, and get rid of any blank cells and any cells which are not being read as numbers - normally a sign of a missing value or a non-numerical character. Why could this be happening? It could be\n\ndue to blank spaces in the text file\ndue to letters where there should be numbers\ndue to characters (“,”, “-”, etc) that shouldn’t really be there\n\nIn general, there will be some detective work required to figure out what’s wrong in our text file. Your best bet is sometimes to open up the data in a text editor, like I’ve done here:\n\nfrom IPython.display import Image\n\ndata_path = \"https://s3.eu-west-2.amazonaws.com/qm2/wk2/data.png\"\nImage(data_path)\n\nThat’s a screenshot of our datafile, opened up in a text editor. As we can see, these numbers are separated by commas and surrounded by quotation marks - this is normal, and what .csv files are supposed to look like. However, there are a lot of commas within the numbers - which makes it easier for people to read, but confuses software. Luckily, Python has a method for dealing with this - the “replace” method.\nUnfortunately, this dataframe is quite messy, so I’m going to have to extract just the columns of data I’m interested in to make it work. I’ll do that by creating a new dataframe:"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#example-cleaning-data",
    "href": "notebooks/W02. Pandas.html#example-cleaning-data",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Example: Cleaning data",
    "text": "Example: Cleaning data\n\nclean = income[['Childless couple, annual income','Couple, two children under 14']]\nclean.head()\n\nWe see those pesky commas. Now we can get on with cleaning up the data:\n\nclean=clean.replace(',', '', regex=True)\n\n# In addition, missing values are sometimes written as '-', in order for Python to understand that it is just a missing numerical \n# value, all '-' need to be replaced with 'NaN'.\nclean = clean.replace('-', 'NaN', regex=True).astype('float')\nclean.head()\n\nExtension: “Regex” refers to “Regular Expression”, which is a way of replacing and cleaning text. It’s a bit beyond the scope of this class, but worth looking into if you’re interested in programming more widely.\nThis seems to have done the job. We’ve also put a line in the code to get rid of dashes - a way that data collectors will sometimes represent missing data. Now let’s plot this."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#asking-more-questions-of-the-data",
    "href": "notebooks/W02. Pandas.html#asking-more-questions-of-the-data",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Asking more questions of the data",
    "text": "Asking more questions of the data\nFor me, this data starts to beg further questions. How would we answer these?\n\nIf the top 20% of income shows such a sharp increase, how do we know that there isn’t a similar uptick within the 1%? We’ve already seen that the meaan of the dataset as a whole is much less than the half the maximum category (it’s 25% of the maximum). What if that’s true within the 1%, and £2,000/week as a fraction of the 0.1%, or the 0.01%?\nHow does this break down for gender, or educational background, or other factors like ethnicity or country of origin?\nWhich parts of the income curve show greater gaps between these subgroups and what might it say about the underlying causal mechanisms?\n\n\nclean.plot()\nplt.title('A Modest Proposal: The fiscal benefits of childbirth')\nplt.xlabel('Percentile')\nplt.ylabel('Income Per Week [GBP]')"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#exercise-1",
    "href": "notebooks/W02. Pandas.html#exercise-1",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Exercise:",
    "text": "Exercise:\nPreviously, we’d examined income gaps between single people and couples (how very romantic). Repeat the above exercise (cleaning and plotting income data) for the columns we used above for single people and childless couples. Reflect and comment on the differences.\n\nprint(\"Enter your code here\")\n\n\nAdd your reflection here."
  },
  {
    "objectID": "notebooks/W02. Pandas.html#homework-selecting-data",
    "href": "notebooks/W02. Pandas.html#homework-selecting-data",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Homework: Selecting data",
    "text": "Homework: Selecting data\nSo far, we’ve dealt with selecting data in a particular row of column by index or label. What if we now want to filter the data by value? For example, let’s say I want to see the data for all Childless couples who earn more than 50,000 (net equivalised) pounds every year. This looks like:\n\nclean = income[['Childless couple, annual income','Couple, two children under 14']]\nclean = clean.replace(',', '', regex=True)\nclean = clean.replace('-', 'NaN', regex=True).astype('float')\nclean[clean['Childless couple, annual income']>50000]\n\nThe key line of code for selection is:\nclean[clean['Childless couple, annual income']>50000]\nLet’s break this down: we’re used to using dataframe[some selection] from earlier. Here “some selection” is\nclean['Childless couple, annual income']>50000\nIn other words, this command is returning a set of indices where that statement is true. We can see this explicitly:\n\nclean['Childless couple, annual income']>50000\n\nSo python is picking the values where this statement is true - i.e. where the ‘Childless couple…’ column has values greater than 50000. Then this selection is passed to the dataframe, and the dataframe shows the correct rows.\nWe won’t dwell on comparative operative, here we’ve used “>” to mean “is greater than”; you can also use:\n\n== to mean ‘is equal to’ [why the double equals?]\n<> or != to mean ‘is not equal to’\n< to mean ‘is less than’\nthe symbol >= to mean ‘is greater than or equal to’\n<= to mean ‘is less than or equal to’"
  },
  {
    "objectID": "notebooks/W02. Pandas.html#exercise-2",
    "href": "notebooks/W02. Pandas.html#exercise-2",
    "title": "Workshop 2: Working With Data In Pandas",
    "section": "Exercise",
    "text": "Exercise\nOn an approporiately labelled graph, plot the incomes of all single adults whose net equivalised income is less than or equal to £10,000. What proportion of the population is this?"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html",
    "href": "notebooks/W03. Spatial Data.html",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "",
    "text": "In this workshop, we will work with data that information about space and time, and show different ways of presenting this data, with the goal of producing fully-fledged maps.\n\n\n\nPlot and summarise spatial data\nCreate simple point maps\nUnderstand the basics of projection"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#downloading-the-data",
    "href": "notebooks/W03. Spatial Data.html#downloading-the-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Downloading the Data",
    "text": "Downloading the Data\nLet’s grab the data we will need this week from our course website and save it into our data folder. If you’ve not already created a data folder then do so using the following command.\nDon’t worry if it generates an error, that means you’ve already got a data folder.\n\n!mkdir data\n\n\n!mkdir data/wk8\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk8/tweet_data.csv -o ./data/wk8/tweet_data.csv\n\n------------------------------\n\nimport pandas\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pylab\n\n%matplotlib inline\nplt.style.use('ggplot')\npylab.rcParams['figure.figsize'] = (10, 8)"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#point-and-areal-data",
    "href": "notebooks/W03. Spatial Data.html#point-and-areal-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Point and Areal Data",
    "text": "Point and Areal Data\nWe’re going to look at some point data, data which has a spatial location but not an extent - this can be contrasted with areal data, where data is reported or represented as covering, or relating to, a specific region geography. An example of this second category would be the released census geography - which, as we saw earlier in the term, is reported on a bespoke areal unit called the Output Area.\nToday we will look at point data. For the purpose, we’ll be looking at some data from twitter - data which has detailed spatial position as well as time and date."
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#the-birds-sing-a-pretty-song",
    "href": "notebooks/W03. Spatial Data.html#the-birds-sing-a-pretty-song",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "The birds sing a pretty song",
    "text": "The birds sing a pretty song\nLet’s start by loading in the twitter data and running head() to take a look at what the dataset contains. The dataset has information about tweeters but not the content of the tweets:\n\ndata_path = \"./data/wk8/tweet_data.csv\"\n\ntweets = pandas.read_csv(data_path, parse_dates=[1], infer_datetime_format=True, encoding = 'latin1')\ntweets.head()"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise",
    "href": "notebooks/W03. Spatial Data.html#exercise",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nWhat does each data column represent?"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#working-with-datetime-data",
    "href": "notebooks/W03. Spatial Data.html#working-with-datetime-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Working with datetime data",
    "text": "Working with datetime data\nDatetime data is a little trickier to work with - it has structure which allows the extraction of hours, minutes, seconds, and so on. For example, we can just take the time part:\n\ntweets['dateT'].dt.time.head()"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-1",
    "href": "notebooks/W03. Spatial Data.html#exercise-1",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nWhat temporal extent does the data cover? How do we need to structure our approach?"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-2",
    "href": "notebooks/W03. Spatial Data.html#exercise-2",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nCreate a new column in the dataframe which stores the “minute” component of the timestamp, and use it to create a histogram of the data over the course of an hour in five minute intervals. Make sure that your graph includes a title and labelled axes.\nYou can also execuate this code in one line if you don’t mind seeing a lot of dots - I’ve not included the parameters to give this some polish - we will leave that as an exercise.\n\ntweets['dateT'].dt.minute.hist()"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#my-first-map",
    "href": "notebooks/W03. Spatial Data.html#my-first-map",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "My First Map",
    "text": "My First Map\nThe beauty of this data is that the data points have x and y values, so plotting them as a scatter graph will give us out first approximation (with caveats) of a map of the data:\n\ntweets.plot(\n    kind='scatter',\n    x='Lon',\n    y='Lat',\n    title=\"Location of Tweets\")\nplt.xlabel(\"Longitude [degrees]\")\nplt.ylabel(\"Latitude [degrees]\")\n\nNotice that we’ve laid out the code so it’s easier to see the multiple arguments in plot() - this is just the same as:\n\ntweets.plot(kind='scatter',x='Lon',y='Lat', title=\"Location of Tweets\")\nplt.xlabel(\"Longitude [degrees]\")\nplt.ylabel(\"Latitude [degrees]\")"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-3",
    "href": "notebooks/W03. Spatial Data.html#exercise-3",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nChange the style of the above map using the optional arguments:\n\nalpha = : to set the opacity - 1 being opaque and 0 being transparent. Set the transparecny so that you can see busy areas and individual points\ncolor=: to set the colour to ‘red’\ns=: so set the point size to 50\n\nSo we have something that looks a bit like a heat map, and even looks a bit Gaussian. Let’s see what a histogram of this data looks like:\n\ntweets['Lat'].hist(bins = 50)\nplt.xlabel(\"Latitude [degrees]\")\nplt.ylabel(\"Number of Tweets\")\n\n\nax = tweets['Lon'].hist(bins=50)\nplt.xlabel(\"Longitude [degrees]\")\nax.set_ylabel(\"Number of Tweets\")"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#the-naivest-projection",
    "href": "notebooks/W03. Spatial Data.html#the-naivest-projection",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "The Naivest Projection",
    "text": "The Naivest Projection\nThe above tweet plot implicitly uses the equirectangular projection, which maps longitude onto the x axis, and latitude onto the y axis. What is the problem with this ?\nProjection is hugely complex and mathematically fiddly - luckily, we’ll be working with packages which mostly do the heavy lifting for us. It’s still worth thinking about projection a bit, as the process of taking points on a sphere and translating that to a flat surface is never a perfect one.\nIf we look at the picture below, then clearly the distance between \\(p\\) and \\(q\\) is the length of the curve on the sphere rather than the straight line between them. The closer \\(p\\) and \\(q\\) are the more the distance is like a straight line, and we can use a linear mapping - i.e. the x coordinate is a linear function of lon, and the y axis is a linear function of latitude.\n\nfrom IPython.display import Image\nImage(\"https://s3.eu-west-2.amazonaws.com/qm2/wk8/great-circle-distance.png\")"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#extension-not-so-great-circles",
    "href": "notebooks/W03. Spatial Data.html#extension-not-so-great-circles",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Extension: Not-so-great circles",
    "text": "Extension: Not-so-great circles\nCalculating great circle distances is the “real” way of figuring out the distance between two points on a sphere is fairly complex. Thankfully, there’s a small angle approximation.\nIf the two points (1 and 2) have latitudes \\(\\phi_1\\) and \\(\\phi_2\\) and longitudes \\(\\lambda_1\\) and \\(\\lambda_2\\), then let \\(\\Delta\\phi = \\phi_1 - \\phi_2\\) and \\(\\Delta\\lambda = \\lambda_1 - \\lambda_2\\) , where \\((\\phi_1,\\lambda_1) ,(\\phi_2,\\lambda_2)\\) are two points given in (latitude, longitude).\nIf \\(\\Delta\\phi\\) and \\(\\Delta\\lambda\\) are small enough, you can calculate the distance \\(D\\) with :\n\\[D = R \\sqrt{(\\Delta\\phi)^2+(cos(\\bar{\\phi})\\Delta\\lambda)^2}\\]\nwhere \\(\\bar{\\phi}\\) is the mean latitude of the two points, \\(\\frac{1}{2}(\\phi_1+\\phi_2)\\), and R is the radius of the earth.\nHow small is small enough if we want to use this approximation? Well, it depends on how much error you want to incur. But generally if the angles are much less than one radian, you’ll incur small errors. Radians, you say? Yes, everything in the above equations assumes angles are expressed in radians. 1 radian is about 57 degrees, but there are more precise definitions, and python has a utility function for converting between the two.\nFor reference, the errors accumulated over the size of London are tens of metres."
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#british-values",
    "href": "notebooks/W03. Spatial Data.html#british-values",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "British Values",
    "text": "British Values\nThe British National Grid provides projected values in metres, so we can get by without doing projections “on the fly” just yet. If we plot these values, it will look pretty similar, for the reasons outlined above - over the few km of London, most projection methods are quite close to the linear mapping we’ve done.\n\nax = tweets.plot(\n    kind='scatter',\n    x='OSGB_Lon',\n    y='OSGB_Lat',\n    title=\"Location of Tweets\")\nax.set_xlabel(\"projected Longitude\")\nax.set_ylabel(\"projected Latitude\")"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-describing-the-data",
    "href": "notebooks/W03. Spatial Data.html#exercise-describing-the-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise: Describing the Data",
    "text": "Exercise: Describing the Data\n\nFind the data centroid (lat, lon)\nCalculate the x, y and total extent of the data, in km (or miles). (Use the projected [OSGB] data for that.)\n\nHint: use commands which capture the maximum, minimum and mean of the data - describe() is a useful one here."
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-4",
    "href": "notebooks/W03. Spatial Data.html#exercise-4",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nHow might we go about calculating the geographical extent which contained 95% of tweets? Assuming the distribution is Gaussian in both variables, estimate a) the latitude the limits which contain 95% of tweets, b) the longitude limits which contain 95% of tweets. Then c) and add lines showing these limits to the tweet graph and d) save the figure as an image using plt.savefig(filename).\nWhat proportion of tweets is held within this box?\nHow do you think the following elements influence the above result?\n\nThe 2D nature of the data\nAsymmetry of the Gaussian (i.e. if \\(\\sigma_x \\neq \\sigma_y\\))\nWhether the data is Gaussian!\nWhat other approaches could you take with this data?\n\n\n\n\n\nImage(\"https://s3.eu-west-2.amazonaws.com/qm2/wk8/tweets.png\")"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#working-in-2d",
    "href": "notebooks/W03. Spatial Data.html#working-in-2d",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Working in 2D",
    "text": "Working in 2D\nIt’s clear that we can learn from 1D about how we can approach 2D data. But there are limitations, and treating 2D as two sets of 1D data doesn’t work for everything. We need to find ways to carry out histograms and other aggregations in 2D - and the first of these is hexbinning."
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#hexbinning",
    "href": "notebooks/W03. Spatial Data.html#hexbinning",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Hexbinning",
    "text": "Hexbinning\nWe can also use a hexbin clustering method, which is similar to binning in a histogram, the more points we have in hexbin the warmer the color. Here, we count the number data points in each hexagons, in the same way that we count the number of data in each bin for 1D data\nLuckily, the code is very easy to execute, and requires only small changes:\n\nax = tweets.plot(\n    kind='hexbin',\n    x='OSGB_Lon', y='OSGB_Lat',\n    gridsize=50,\n    title=\"Tweet Density (Hex Bin)\",\n    cmap='coolwarm',\n    )\nplt.xlabel(\"projected Longitude\")\nplt.ylabel(\"projected Latitude\")\n\nPossible values are: Spectral, summer, coolwarm, Wistia_r, pink_r, Set1, Set2, Set3, brg_r, Dark2, prism, PuOr_r, afmhot_r, terrain_r, PuBuGn_r, RdPu, gist_ncar_r, gist_yarg_r, Dark2_r, YlGnBu, RdYlBu, hot_r, gist_rainbow_r, gist_stern, PuBu_r, cool_r, cool, gray, copper_r, Greens_r, GnBu, gist_ncar, spring_r, gist_rainbow, gist_heat_r, Wistia, OrRd_r, CMRmap, bone, gist_stern_r, RdYlGn, Pastel2_r, spring, terrain, YlOrRd_r, Set2_r, winter_r, PuBu, RdGy_r, spectral, rainbow, flag_r, jet_r, RdPu_r, gist_yarg, BuGn, Paired_r, hsv_r, bwr, cubehelix, Greens, PRGn, gist_heat, spectral_r, Paired, hsv, Oranges_r, prism_r, Pastel2, Pastel1_r, Pastel1, gray_r, jet, Spectral_r, gnuplot2_r, gist_earth, YlGnBu_r, copper, gist_earth_r, Set3_r, OrRd, gnuplot_r, ocean_r, brg, gnuplot2, PuRd_r, bone_r, BuPu, Oranges, RdYlGn_r, PiYG, CMRmap_r, YlGn, binary_r, gist_gray_r, Accent, BuPu_r, gist_gray, flag, bwr_r, RdBu_r, BrBG, Reds, Set1_r, summer_r, GnBu_r, BrBG_r, Reds_r, RdGy, PuRd, Accent_r, Blues, autumn_r, autumn, cubehelix_r, nipy_spectral_r, ocean, PRGn_r, Greys_r, pink, binary, winter, gnuplot, RdYlBu_r, hot, YlOrBr, coolwarm_r, rainbow_r, Purples_r, PiYG_r, YlGn_r, Blues_r, YlOrBr_r, seismic, Purples, seismic_r, RdBu, Greys, BuGn_r, YlOrRd, PuOr, PuBuGn, nipy_spectral, afmhot"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#extension-splitting-mapping-data-by-time",
    "href": "notebooks/W03. Spatial Data.html#extension-splitting-mapping-data-by-time",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Extension: Splitting Mapping Data by Time",
    "text": "Extension: Splitting Mapping Data by Time\nIn the next section, we use both space and time to show different geographical distributions at different times. We’ll select on index, splitting the dataset in two.\n\nearly = tweets[:750]\nlate = tweets[750:len(tweets)]\n\n\nearly.head()\n\n\nlate.head()"
  },
  {
    "objectID": "notebooks/W03. Spatial Data.html#exercise-5",
    "href": "notebooks/W03. Spatial Data.html#exercise-5",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nPlot both sets of tweets onto the same axes so they can be compared. Try and make your plot look like the image below.\n\nImage(\"https://s3.eu-west-2.amazonaws.com/qm2/wk8/two_times.png\")\n\n\n\n\nWe can visually inspect the spatial plots of the two time frames using hexbin plots; in this case there’s not much to see…\n\nax = early.plot(\n    kind='hexbin',\n    x='OSGB_Lon', \n    y='OSGB_Lat',\n    gridsize=50,\n    title=\"Tweet Density (Hex Bin)\",\n    cmap='coolwarm',\n    )\nplt.xlabel(\"projected Longitude\")\nplt.ylabel(\"projected Latitude\")\n\n\nax = late.plot(\n    kind='hexbin',\n    x='OSGB_Lon', \n    y='OSGB_Lat',\n    gridsize=50,\n    title=\"Tweet Density (Hex Bin)\",\n    cmap='coolwarm',\n    )\nplt.xlabel(\"projected Longitude\")\nplt.ylabel(\"projected Latitude\")"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html",
    "href": "notebooks/W04. Natural Language Processing.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Today we’ll be using the Natural Language Tool Kit package nltk, which will allow us to split (clean) text into words, parts of speech, and sentences, and plot word occurrence and frequency.\nAims\n\nto work with nltk and some standard corpus texts\nto tokenise by word and sentence\nto plot word occurrence and frequency\nto filter by parts of speech"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#downloading-the-data",
    "href": "notebooks/W04. Natural Language Processing.html#downloading-the-data",
    "title": "Natural Language Processing",
    "section": "Downloading the Data",
    "text": "Downloading the Data\nLet’s grab the data we will need this week from our course website and save it into our data folder. If you’ve not already created a data folder then do so using the following command.\nDon’t worry if it generates an error, that means you’ve already got a data folder.\n\n#Make a ./data directory\n!pip install nltk\n!mkdir data\n\nRequirement already satisfied: nltk in /Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages (3.7)\nRequirement already satisfied: click in /Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (8.0.1)\nRequirement already satisfied: joblib in /Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: regex>=2021.8.3 in /Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (2022.9.13)\nRequirement already satisfied: tqdm in /Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (4.61.0)\nWARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\nYou should consider upgrading via the '/Users/ollieballinger/.pyenv/versions/3.9.5/bin/python3.9 -m pip install --upgrade pip' command.\nmkdir: data: File exists\n\n\n\n#Make a ./data/wk7 directory\n!mkdir data/wk7\n\n#Download the data into this directory\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelA.txt -o ./data/wk4/PanelA.txt\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelA2.txt -o ./data/wk4/PanelA2.txt\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelB.txt -o ./data/wk4/PanelB.txt\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelC.txt -o ./data/wk4/PanelC.txt\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelD.txt -o ./data/wk4/PanelD.txt\n\nmkdir: data/wk7: File exists\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  6374  100  6374    0     0  87376      0 --:--:-- --:--:-- --:--:-- 92376\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  4091  100  4091    0     0  36609      0 --:--:-- --:--:-- --:--:-- 39336\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 10903  100 10903    0     0  63110      0 --:--:-- --:--:-- --:--:-- 64514\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  8140  100  8140    0     0  32892      0 --:--:-- --:--:-- --:--:-- 336306\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5801  100  5801    0     0  45085      0 --:--:-- --:--:-- --:--:-- 46782\n\n\n\nimport pylab\n%matplotlib inline\npylab.rcParams['figure.figsize'] = (10., 8.)"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#dirty-words",
    "href": "notebooks/W04. Natural Language Processing.html#dirty-words",
    "title": "Natural Language Processing",
    "section": "Dirty Words",
    "text": "Dirty Words\nText often comes ‘unclean’ either containing tags such as HTML (or XML), or has other issues, but fortunately we will be using ‘clean’ sources, at least initially. Be cautious when committing to a text analysis project - you may spend a great deal of time tidying up your text.\nThe kind of analysis we will be doing reqires tokenizing a text, and tagging individual words. Tokenizing means splitting the text into individul sentences or individual words, while tagging means classifying each word according to a POS (Parts Of Speech) classification."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#the-castle-of-aaargh",
    "href": "notebooks/W04. Natural Language Processing.html#the-castle-of-aaargh",
    "title": "Natural Language Processing",
    "section": "The Castle of Aaargh",
    "text": "The Castle of Aaargh\nWe will first experiment with nltk and its built in corpus texts. We’ll work with some Monty Python, beloved of comedy bores for half a century"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#setup",
    "href": "notebooks/W04. Natural Language Processing.html#setup",
    "title": "Natural Language Processing",
    "section": "Setup",
    "text": "Setup\n\ninstall nltk through package manager, or the command line\nimport nltk\ntype nltk.download(‘book’). This will automatically download the books into our workspace\n\n\n#nltk: natural language processing toolkit\nimport nltk\n\n\n#Download the sample books\nnltk.download('book')\n\n[nltk_data] Downloading collection 'book'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package brown to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package chat80 to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package timit to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package udhr to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /Users/ollieballinger/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection book\n\n\nTrue\n\n\nNow, we import the sample texts. You’ll notice that text6 is “Monty Python and the Holy Grail”, as promised. Presumably this was compiled pre-Spamalot.\n\n#Import all features from nltk.book\nfrom nltk.book import *\n\nLet’s look at the object text6; we can look at the first few words..\n\n#View the first ten words of text6\ntext6[1:10]\n\n['1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop']\n\n\nWhich are presumably stage directions rather than dialogue. How many words/symbols are in the text?\n\n#View the length of the list\nlen(text6)\n\n16967\n\n\nBear in mind that a lot of the functions we will carry out rely on this being a text object - we’ll start to think about how we use free text and convert it to a Text object later.\nWe can now start to do some slightly more sophisticated work; for example, a dispersion plot to see where words appear. Let’s give it a few keywords that those familiar with …The Holy Grail might recognise:\n\ntext6.dispersion_plot(['Grail','rabbit','Knights','Ni','castle'])\n\n\n\n\nAnd we can easily count how many times a word appears.\n\ntext6.count('Ni')\n\n47\n\n\nOr the words which most commonly appear together:\n\ntext6.collocations()\n\nBLACK KNIGHT; clop clop; HEAD KNIGHT; mumble mumble; Holy Grail;\nsqueak squeak; FRENCH GUARD; saw saw; Sir Robin; Run away; CARTOON\nCHARACTER; King Arthur; Iesu domine; Pie Iesu; DEAD PERSON; Round\nTable; clap clap; OLD MAN; dramatic chord; dona eis"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#exercise-from-hells-heart-i-stab-at-thee",
    "href": "notebooks/W04. Natural Language Processing.html#exercise-from-hells-heart-i-stab-at-thee",
    "title": "Natural Language Processing",
    "section": "Exercise: From Hell’s Heart I Stab at Thee",
    "text": "Exercise: From Hell’s Heart I Stab at Thee\nFrom Moby Dick, find out - Where the narrator Ishmael, Captain Ahab and his Nemesis are mentioned. When do each enter the story? Where do they have most emphasis? - Which parts of the books appear to take place at sea, and points where their ship is wrecked or sinking (spoilers) - two significant places in the story (HINT: use collocations)\nLet’s now look at carrying out the full process of importing and working with text data."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#making-an-impact",
    "href": "notebooks/W04. Natural Language Processing.html#making-an-impact",
    "title": "Natural Language Processing",
    "section": "Making an IMPACT",
    "text": "Making an IMPACT\nAs part of the REF2014 exercise, universities reported on the Impact their research activities had on the world. Their Impact Case Studies were subsequently made available by HEFCE. What sort of information do they contain? How do universities frame “impact”? All of this data is available via the REF website.\nA little context: I’ve included examples from the four panels used by HEFCE. Broadly speaking, Panel A is health, bioscience and medicine, B is physical science and engineering, C is social science, and D is humanities - the full categories are visible here: http://www.ref.ac.uk/panels/unitsofassessment/\nLet’s first look at random sample from Panel A:\n\n#Data path to file\ndata_path = \"./data/wk7/PanelA.txt\"\n\nwith open(data_path) as file:\n    data = file.read()\nprint(data)\n\nFileNotFoundError: [Errno 2] No such file or directory: './data/wk7/PanelA.txt'\n\n\nWe could tokenize this into sentences:\n\nsentences = nltk.sent_tokenize(data)\nsentences[1:5]\n\n['Researchers developed a practice development framework for implementing and assessing the delivery of evidence-based practice in 82 UK health and social care units during the impact period.',\n 'Benefits to staff include better communication and team structure.',\n 'Benefits to patients include higher standards of cleanliness, privacy and dignity, as well as a decrease in length of hospital stays and appointment waiting times.',\n 'Delivery has extended to cover entire NHS Trusts serving a resident population of over 3.5 million, social services departments and third sector organisations across the south of England and beyond.']\n\n\nOr into individual words; generally, it may be useful to retain sentences, so we can see where two words are in the same sentence, for example - but we’ll be doing something simpler:\n\ntokens = nltk.word_tokenize(data)\n\n\ntokens[1:20]\n\n['*',\n '*',\n '*',\n '*',\n 'title_inst_10000824-u_3-case_43396',\n '*',\n 'UKPRN_10000824',\n '*',\n 'uoa_3',\n '*',\n 'ID_43396',\n '*',\n 'panel_A',\n 'Bournemouth',\n 'University',\n '(',\n 'BU',\n ')',\n 'has']\n\n\nTokenising is a process which has many subtleties and corner-cases, and you may want to proceed in a more fine-grained way for some texts:\nhttp://nltk.org/api/nltk.tokenize.html\nLet’s now convert this into a Text object, which will allow us to analyse other aspects of the text. For example, we can look at collocations, words which commonly appear together. This may help to provide context.\n\nsimple_text = nltk.Text(tokens)\nsimple_text.collocations()\n\npractice development; Foundation Trust; NHS Foundation; 3.5 million;\nPoole Hospital; Trusts serving; resident population; social care;\nHospital NHS; waiting times; action plan; development process; patient\nprivacy; user journey; better communication; cultural change; visiting\ntimes; team members; services departments; evidence-based practice\n\n\nWe can create a dispersion plot - although in this case, it tells us a limited amount…\n\nsimple_text.dispersion_plot(['NHS', 'evidence', 'practice', 'Hospital'])\n\n\n\n\nEven from this, we get a sense of the work this unit does, and its impacts on the world. But what are the most 20 common words used? To find this out, we produce a Frequency Distribution (FreqDist) object. This has an implicit loop - the for statement is telling python to look through all the words in ‘tokens’ and seeing how often they occur. The .lower() command converts them all to lower case for comparison, so it will flag up upper and lower case occurrences of the word.\n\nfd = nltk.FreqDist(word.lower() for word in tokens)\nfd.plot(20)\n\n\n\n\n<AxesSubplot:xlabel='Samples', ylabel='Counts'>\n\n\nNot very helpful - this includes all kinds of junk, and tells us that “and” is very common. Not very interesting. Let’s try a bit harder and identify Parts of Speech."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#pos",
    "href": "notebooks/W04. Natural Language Processing.html#pos",
    "title": "Natural Language Processing",
    "section": "POS",
    "text": "POS\nParts of speech indicate whether something is a noun, a verb, adjective, and so on. In nltk, we can use the pos_tag command, which will identify which word belongs to which part of speech.\n\ntagged = nltk.pos_tag(tokens)\ntagged[0:10]\n\n[('*', 'JJ'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('title_inst_10000824-u_3-case_43396', 'JJ'),\n ('*', 'NNP'),\n ('UKPRN_10000824', 'NNP'),\n ('*', 'NNP'),\n ('uoa_3', 'JJ')]\n\n\n‘NNP’ refers to Proper Noun, Singular; you can find the full list of Parts of Speech here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n\npermitted_tags = set([\n    'NN',\n    'NNS'\n])"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#one-for-all",
    "href": "notebooks/W04. Natural Language Processing.html#one-for-all",
    "title": "Natural Language Processing",
    "section": "One FOR All",
    "text": "One FOR All\nWe’ve so far managed to avoid this staple of programming, the FOR loop - and we’re not going to delve too deeply into it in the last class of term. Of course, you probably came across FOR loops and IF statements when you worked through the prerequisites for the module, but that feels like a long time ago…\nWe do use FOR and IF here, and it’s worth understanding a bit about what it means, even if you don’t intend to use it a lot yourself. In the next piece of code, we set up fd, a new object which will record frequency distribution information. Then we use a FOR loop\n`for bit in tagged: …’\nThis goes through every element of tagged one at a time - and each element is called ‘bit’ for the purposes of this loop. Then, for each ‘bit’, we check that it has one of the permitted tags, and make sure it’s at least 3 characters long - shorter words probably aren’t all that relevant in this case:\n`if bit[1] in permitted_tags and len(bit[0])>2:’\nnote that the and means both of these have to be true - if both are true, only then does the following statement execute:\n`fd[bit[0]] = fd[bit[0]] + 1’\nwhich increases the count for that word. So, this code increases the count for a word iff (if and only if) its at least 3 characters long, and it’s of the correct tag (Noun, Singular or Plural)."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#double-indentity",
    "href": "notebooks/W04. Natural Language Processing.html#double-indentity",
    "title": "Natural Language Processing",
    "section": "Double Indentity",
    "text": "Double Indentity\nOne final remark: we haven’t dealt with indents much in python, but indenting the code like below, after the for statement, and again after the if statement, is the way that python knows it’s dealing with a loop (for) and a conditional (if). It’s also the way python deals with defining new functions, but that’s not something you will need to do. This is just a pointer - if your code doesn’t work, check the colons are there (:) and the indenting is too.\nOn with the show - as promised, this creates a word frequency graph of nouns:\n\nfd = nltk.FreqDist()\n\nfor bit in tagged:\n    if bit[1] in permitted_tags and len(bit[0])>2:\n        fd[bit[0]] = fd[bit[0]] + 1\n        \nfd.plot(20)\n\n\n\n\n<AxesSubplot:xlabel='Samples', ylabel='Counts'>\n\n\nWe start to get a sense of the impact - ‘care’, ‘practice’, and ‘services’ all feature heavily.\nLet’s now look at another randomly chosen example from Panel A:\n\ndata_path = \"./data/wk7/PanelA2.txt\"\n\nwith open(data_path) as file:\n    data = file.read()\ntokens = nltk.word_tokenize(data)\nsimple_text.collocations()\nsimple_text = nltk.Text(tokens)\ntagged = nltk.pos_tag(tokens)\nfd = nltk.FreqDist()\n\nfor bit in tagged:\n    if bit[1] in permitted_tags and len(bit[0])>2:\n        fd[bit[0]] = fd[bit[0]] + 1\n        \nfd.plot(20)\n\npractice development; Foundation Trust; NHS Foundation; 3.5 million;\nPoole Hospital; Trusts serving; resident population; social care;\nHospital NHS; waiting times; action plan; development process; patient\nprivacy; user journey; better communication; cultural change; visiting\ntimes; team members; services departments; evidence-based practice\n\n\n\n\n\n<AxesSubplot:xlabel='Samples', ylabel='Counts'>\n\n\nA very different set of words, clearly geared towards language therapy, and working directly with patients. Perhaps if we looked at the slightly less common words, we’d see links between these submissions - for example, we see communication appearing in both. Not a huge surprise, if we’re talking about public impact, but students of the public role of the university might start to wonder about the distinctions between communication and engagement."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#exercise",
    "href": "notebooks/W04. Natural Language Processing.html#exercise",
    "title": "Natural Language Processing",
    "section": "Exercise",
    "text": "Exercise\nRepeat this for the examples from Panels B, C and D - what trends and keywords appear? What use do the collocations have? What do different parts of speech (e.g. verbs or proper nouns) tell you about the text?\nIf we wanted to analyse the sector as a whole, we would want to analyse Impact statements en masse - and we would hope that this would draw out links across differnt statements from different centres and universities, and even in different panels."
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#working-with-larger-text-datasets",
    "href": "notebooks/W04. Natural Language Processing.html#working-with-larger-text-datasets",
    "title": "Natural Language Processing",
    "section": "Working with larger text datasets",
    "text": "Working with larger text datasets\nWorking with larger text corpora starts to get slow. At this point, we will look at a body of text we have previously tagged up. The file is “The Nameless City” by H. P. Lovecraft, a horror author from the early 20th century.\n\nimport pickle\nimport requests\nfrom urllib.request import urlopen\n\nThis may take a little while - so wait for the task to run:\n\n# Loading the tokenized and tagged file. \ntagged = pickle.load(urlopen(\"https://s3.eu-west-2.amazonaws.com/qm2/wk7/lovecraft_tagged.pickle\"), encoding='latin1')\n\n#How many sentences do we have?\nlen(tagged)\n\n18513\n\n\nThis is tokenised by sentence - and there are 18,513 of them. That would have taken a long time to tag up. If you’re interested, this is how you take a set of sentences and tag them with Part of Speech:\n\nexampleSentences = nltk.sent_tokenize(data)\n  \nexampleTagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sentences]\n\nNote that the second line is running an implicit for loop through every sentence, and tagging each word.\n\n# first sentence.\nexampleTagged[0]\n\n[('*', 'JJ'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('*', 'NNP'),\n ('title_inst_10000824-u_3-case_43396', 'JJ'),\n ('*', 'NNP'),\n ('UKPRN_10000824', 'NNP'),\n ('*', 'NNP'),\n ('uoa_3', 'JJ'),\n ('*', 'NNP'),\n ('ID_43396', 'NNP'),\n ('*', 'NNP'),\n ('panel_A', 'NN'),\n ('Bournemouth', 'NNP'),\n ('University', 'NNP'),\n ('(', '('),\n ('BU', 'NNP'),\n (')', ')'),\n ('has', 'VBZ'),\n ('facilitated', 'VBN'),\n ('improvements', 'NNS'),\n ('to', 'TO'),\n ('health', 'NN'),\n ('and', 'CC'),\n ('social', 'JJ'),\n ('care', 'NN'),\n ('practice', 'NN'),\n ('through', 'IN'),\n ('cultural', 'JJ'),\n ('change', 'NN'),\n ('in', 'IN'),\n ('care', 'NN'),\n ('provision', 'NN'),\n ('.', '.')]\n\n\nAn impressive start! Let’s again build our word frequency chart. Note now that we have an extra layer of FOR - we need to look at each sentence in the text; at each word in each sentence; and then check each word to see whether it is of an allowed type.\n\nfd = nltk.FreqDist()\n\npermitted_tags = set([\n    'JJS',\n    'FW',\n    'NN',\n    'NNS',\n    'NNP',\n    'NNPS',\n    'UH',\n])\nfor sentence in tagged:\n    for word in sentence:\n        if word[1] in permitted_tags:\n                fd[word[0]] = fd[word[0]] + 1\nfd.plot(20)\n\n\n\n\n<AxesSubplot:xlabel='Samples', ylabel='Counts'>\n\n\nNow that we’ve produced counts for all words which conform to our list of tags, we can quickly see how frequently common words appear; because we have tokenized by sentence, we have to do this with a slightly different mechanism - run through the words of interest and see how many occurrences appear in the Frequency Distribution object, fd. Again, we’re sneaking in a FOR loop to run through these.\n\nfor word in ['space', 'nameless', 'mad', 'dread', 'fear', 'cthulhu', 'necronomicon', 'caring']:\n    print(word, fd[word])\n\nspace 279\nnameless 174\nmad 101\ndread 49\nfear 282\ncthulhu 40\nnecronomicon 54\ncaring 0\n\n\nSo far, we’ve completely avoided the use of pandas - but we can put this data into a pandas dataframe very easily, and use the built-in graphing methods to change the style of our graph.\nWe feed in fd.keys() - the words - and fd.values(), the wordcount.\n\n#Convert to list so subscriptable\nlist(fd.keys())[1:10]\n\n['city', 'valley', 'moon', 'afar', 'sands', 'parts', 'corpse', 'grave', 'fear']\n\n\n\n#Convert to list so subscriptable\nlist(fd.values())[1:10]\n\n[456, 116, 276, 27, 17, 108, 36, 80, 282]\n\n\n\nimport pandas as pd\n\n#Convert to list for use in creating dataframe\ndf = pd.DataFrame({'items': list(fd.keys()), 'counts': list(fd.values())})\ndf.head()\n\n/Users/ollieballinger/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n\n\n\n\n\n\n  \n    \n      \n      items\n      counts\n    \n  \n  \n    \n      0\n      nameless\n      174\n    \n    \n      1\n      city\n      456\n    \n    \n      2\n      valley\n      116\n    \n    \n      3\n      moon\n      276\n    \n    \n      4\n      afar\n      27\n    \n  \n\n\n\n\nLet’s now arrange them in order of appearance - the most common at the top.\n\ndf = df.sort_values(by='counts',ascending=False)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      items\n      counts\n    \n  \n  \n    \n      137\n      time\n      960\n    \n    \n      18\n      man\n      884\n    \n    \n      192\n      things\n      875\n    \n    \n      38\n      night\n      725\n    \n    \n      215\n      thing\n      667\n    \n  \n\n\n\n\nWe now have a DataFrame with certain word tags sorted by decreasing frequency. Let’s plot this in a bar graph; we will use df[1:50] to select the most common 50 words.\n\nimport matplotlib.pyplot as plt\n\n\nplt.style.use('ggplot')\ndf[1:50].plot(kind='bar', x='items', y='counts', legend=False)\nplt.xlabel('Word')\nplt.ylabel('Word Count')\nplt.title('Word counts for H.P. Lovecraft\\'s \\\"The Nameless City\\\"')\nplt.axhline(df['counts'].mean(), color='#2222ff')\n\n<matplotlib.lines.Line2D at 0x1888076a0>"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#exercise-1",
    "href": "notebooks/W04. Natural Language Processing.html#exercise-1",
    "title": "Natural Language Processing",
    "section": "Exercise",
    "text": "Exercise\nWhat is the percentage of words that appear exactly once in the entire text? (Hint : FreqDist objects have a method called ‘hapaxes’)"
  },
  {
    "objectID": "notebooks/W04. Natural Language Processing.html#exercise-2",
    "href": "notebooks/W04. Natural Language Processing.html#exercise-2",
    "title": "Natural Language Processing",
    "section": "Exercise",
    "text": "Exercise\nThe words of our ex Prime Minister, David Cameron:\n1. Select one of Cameron's speeches.\n2. Sentence and word tokenize it.\n3. POS tag it.\n4. Create noun and adjective histograms of the 20 most frequent words.\n(notice that there are several POS for each.)\n5. Create dispersion plots for these nouns and adjectives. \n6. What is the percentage of words that are adjectives in the speech?\n\n# read a raw text from a remote location. \nspeech = requests.get('https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt').text\n\n\nSpeeches\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-election-victory-speech-2010.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006b.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2007.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2008.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2013.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2012.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2011.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2010.txt\nhttps://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2009.txt\n\nspeech\n\n'It\\x92s a huge honour to be standing before you as leader of the Conservative Party.\\r\\n\\r\\n \\r\\n\\r\\nAnd first of all I want to thank you for the support you\\x92ve given me in the past ten months.\\r\\n\\r\\n \\r\\n\\r\\nIt\\x92s been a time of great change.\\r\\n\\r\\n \\r\\n\\r\\nI\\x92m already on my second leader of the Liberal Democrats.\\r\\n\\r\\n \\r\\n\\r\\nBefore long I\\x92ll be on to my second Labour Prime Minister.\\r\\n\\r\\n \\r\\n\\r\\nSoon I\\x92ll be the longest-serving leader of a major British political party.\\r\\n\\r\\n \\r\\n\\r\\nI wanted this job for a very simple reason.\\r\\n\\r\\n \\r\\n\\r\\nI love this country.\\r\\n\\r\\n \\r\\n\\r\\nI have great ambitions for our future.\\r\\n\\r\\n \\r\\n\\r\\nAnd I want the Party I love\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85to serve the country I love\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85in helping Britain be the best that it can.\\r\\n\\r\\n \\r\\n\\r\\nWe need to change in order to have that chance.\\r\\n\\r\\n \\r\\n\\r\\nYou cannot shape the future if you\\x92re stuck in the past.\\r\\n\\r\\n \\r\\n\\r\\nYou knew that.\\r\\n\\r\\n \\r\\n\\r\\nAnd that\\x92s why you voted for change.\\r\\n\\r\\n \\r\\n\\r\\nI believe we can all be proud of what we\\x92ve achieved these past ten months.\\r\\n\\r\\n \\r\\n\\r\\nPeople looking at us with new interest.\\r\\n\\r\\n \\r\\n\\r\\n25,000 new members.\\r\\n\\r\\n \\r\\n\\r\\nAnd in our first electoral test, in the local elections, we won forty per cent of the vote.\\r\\n\\r\\n \\r\\n\\r\\nLet\\x92s hear it for our fantastic local councillors who worked so hard and won so well.\\r\\n\\r\\n \\r\\n\\r\\nTony Blair says it\\x92s all style and no substance.\\r\\n\\r\\n \\r\\n\\r\\nIn fact he wrote me a letter about it.\\r\\n\\r\\n \\r\\n\\r\\nDear Kettle\\x85\\r\\n\\r\\n \\r\\n\\r\\nYou\\x92re black.\\r\\n\\r\\n \\r\\n\\r\\nSigned, Pot.\\r\\n\\r\\n \\r\\n\\r\\nWhat a nerve that man has got.\\r\\n\\r\\n \\r\\n\\r\\nIn the whole of the last year, there is only one substantial thing that the Labour Party has achieved for our country.\\r\\n\\r\\n \\r\\n\\r\\nTheir education reforms.\\r\\n\\r\\n \\r\\n\\r\\nRight now, across the country, trust schools are being prepared with greater freedoms to teach children the way teachers and parents want.\\r\\n\\r\\n \\r\\n\\r\\nThe only reason \\x96 the only reason - that\\x92s happening is because the Conservative Party did the right thing and took the legislation through the House of Commons.\\r\\n\\r\\n \\r\\n\\r\\nI\\x92m proud of that \\x96 proud of us, for putting the future of our children before party politics.\\r\\n\\r\\n \\r\\n\\r\\nAnother sign of our changing fortunes is the impressive array of speakers who have come to join us at our conference this year.\\r\\n\\r\\n \\r\\n\\r\\nSENATOR McCAIN\\r\\n\\r\\n \\r\\n\\r\\nAnd I\\x92d like to pay a special tribute to one in particular.\\r\\n\\r\\n \\r\\n\\r\\nHe\\x92s a man who knows about leadership.\\r\\n\\r\\n \\r\\n\\r\\nHe\\x92s endured hardship that\\x92s unimaginable to many of us here.\\r\\n\\r\\n \\r\\n\\r\\nAnd he\\x92s fought battles for principles that we all admire.\\r\\n\\r\\n \\r\\n\\r\\nWho knows what the future may hold?\\r\\n\\r\\n \\r\\n\\r\\nBut John, I for one would be proud to see you \\x96 a great American and a great friend to Britain - as leader of the free world.\\r\\n\\r\\n \\r\\n\\r\\nCOLLEAGUES\\r\\n\\r\\n \\r\\n\\r\\nI\\x92d also like to pay tribute to my colleagues who have spoken already today.\\r\\n\\r\\n \\r\\n\\r\\nA year ago, David Davis and I were rivals.\\r\\n\\r\\n \\r\\n\\r\\nToday we\\x92re partners.\\r\\n\\r\\n \\r\\n\\r\\nHe has given me the most fantastic support over these past ten months.\\r\\n\\r\\n \\r\\n\\r\\nIdeas, energy, advice.\\r\\n\\r\\n \\r\\n\\r\\nHe has not only helped bring this Party together\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85he has helped take our Party in the right direction, and I want to thank him for all he\\x92s done.\\r\\n\\r\\n \\r\\n\\r\\nAnd I\\x92m proud to work with another man who is a brave politician, a wise counsellor and a great Conservative.\\r\\n\\r\\n \\r\\n\\r\\nA man who would be a Foreign Secretary that this country could be truly proud of: William Hague.\\r\\n\\r\\n \\r\\n\\r\\nThen there\\x92s Francis.\\r\\n\\r\\n \\r\\n\\r\\nI know Francis likes to pretend that everything is doom and gloom.\\r\\n\\r\\n \\r\\n\\r\\nHe\\x92s always talking about the mountain we have to climb.\\r\\n\\r\\n \\r\\n\\r\\nHe\\x92s so gloomy, he makes Gordon Brown look like a ray of sunshine.\\r\\n\\r\\n \\r\\n\\r\\nBut Francis, you\\x92re doing a great job.\\r\\n\\r\\n \\r\\n\\r\\nLABOUR SPLITS AND BACKSTABBING\\r\\n\\r\\n \\r\\n\\r\\nOf course Francis has long told us to avoid the point-scoring and name-calling that can give politics such a bad name.\\r\\n\\r\\n \\r\\n\\r\\nHe\\x92s right.\\r\\n\\r\\n \\r\\n\\r\\nBut we didn\\x92t bargain on the Labour Party.\\r\\n\\r\\n \\r\\n\\r\\nFirst Gordon said he could never trust Tony again, then Tony called Gordon a blackmailer.\\r\\n\\r\\n \\r\\n\\r\\nCharles said Gordon was stupid, then John popped up and said no, Tony was stupid.\\r\\n\\r\\n \\r\\n\\r\\nCharles called Gordon a deluded control freak.\\r\\n\\r\\n \\r\\n\\r\\nAnd a member of the Cabinet said \\x93it would be an absolute effing disaster\\x94 if Gordon got to No.10.\\r\\n\\r\\n \\r\\n\\r\\nThat was just the husbands.\\r\\n\\r\\n \\r\\n\\r\\nWhen I look at these Labour ministers I ask myself how much time they\\x92re worrying about their own jobs\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85and how much time they\\x92re worrying about NHS, about crime, about our troops in Iraq and Afghanistan.\\r\\n\\r\\n \\r\\n\\r\\nYou only have to ask the question to know what the answer is.\\r\\n\\r\\n \\r\\n\\r\\nAnd there are months more of it still to come.\\r\\n\\r\\n \\r\\n\\r\\nMonths of infighting, instability, indecision, jockeying for position\\x85\\r\\n\\r\\n \\r\\n\\r\\nThey said it would be a \\x93stable and orderly transition.\\x94\\r\\n\\r\\n \\r\\n\\r\\nYeah, right.\\r\\n\\r\\n \\r\\n\\r\\nLike they said \\x9324 hours to save the NHS\\x94, \\x93education education education.\\x94\\r\\n\\r\\n \\r\\n\\r\\nThese are the things they should be fighting for, but they\\x92re too busy fighting each other.\\r\\n\\r\\n \\r\\n\\r\\nOUR RESPONSIBILITY\\r\\n\\r\\n \\r\\n\\r\\nSo we have a great responsibility.\\r\\n\\r\\n \\r\\n\\r\\nTo set out a clear, united and credible alternative.\\r\\n\\r\\n \\r\\n\\r\\nWith some elections, you just know the result before a single vote has been cast.\\r\\n\\r\\n \\r\\n\\r\\nWe were never going to win in 1997.\\r\\n\\r\\n \\r\\n\\r\\nPeople wanted change.\\r\\n\\r\\n \\r\\n\\r\\nI remember it well.\\r\\n\\r\\n \\r\\n\\r\\nI fought Stafford.\\r\\n\\r\\n \\r\\n\\r\\nAnd Stafford fought back.\\r\\n\\r\\n \\r\\n\\r\\nLabour were never going to win in 1983 when they offered Michael Foot as Prime Minister.\\r\\n\\r\\n \\r\\n\\r\\nOther elections are wide open.\\r\\n\\r\\n \\r\\n\\r\\nAnd the next election will be one of those.\\r\\n\\r\\n \\r\\n\\r\\nBut we will not win, nor deserve to win, without a clear purpose and a proper plan.\\r\\n\\r\\n \\r\\n\\r\\nWe must learn from Labour\\x92s big mistake.\\r\\n\\r\\n \\r\\n\\r\\nWhen Tony Blair won his first election, he had only one clear purpose: to win a second term.\\r\\n\\r\\n \\r\\n\\r\\nEven now he says that the only legacy \\x96 the only legacy - that really matters to him is Labour winning a fourth term.\\r\\n\\r\\n \\r\\n\\r\\nBack in 1997, he had no proper plan.\\r\\n\\r\\n \\r\\n\\r\\nNo real understanding of how to make change happen.\\r\\n\\r\\n \\r\\n\\r\\nHe had good intentions.\\r\\n\\r\\n \\r\\n\\r\\nBut he hadn\\x92t worked out how to deliver them.\\r\\n\\r\\n \\r\\n\\r\\nSo New Labour went round and round in circles.\\r\\n\\r\\n \\r\\n\\r\\nThey abolished grant maintained schools - and now they\\x92re trying to recreate them.\\r\\n\\r\\n \\r\\n\\r\\nThey reversed our NHS reforms - and now they\\x92re trying to bring them back.\\r\\n\\r\\n \\r\\n\\r\\nRoad building \\x96 cancelled, then reinstated.\\r\\n\\r\\n \\r\\n\\r\\nThey wasted time, wasted money, wasted the country\\x92s goodwill.\\r\\n\\r\\n \\r\\n\\r\\nOnly now, after nine years, does Tony Blair seem clear about his purpose.\\r\\n\\r\\n \\r\\n\\r\\nWell I\\x92m sorry Mr Blair.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s nine years too late.\\r\\n\\r\\n \\r\\n\\r\\nTHIS WEEK\\r\\n\\r\\n \\r\\n\\r\\nWe won\\x92t make the same mistake.\\r\\n\\r\\n \\r\\n\\r\\nOn Wednesday, the last day of our conference, I want to talk in detail about the important issues we face as a nation \\x96 and what our response will be.\\r\\n\\r\\n \\r\\n\\r\\nBut today, on this first day of our conference, I\\x92d like to set the scene for our discussions this week.\\r\\n\\r\\n \\r\\n\\r\\nI want to explain how we will arrive at the next election knowing exactly what we want to do, and how we\\x92re going to do it.\\r\\n\\r\\n \\r\\n\\r\\nMy argument is based on a simple analogy.\\r\\n\\r\\n \\r\\n\\r\\nGetting ready for the responsibility of government is like building a house together.\\r\\n\\r\\n \\r\\n\\r\\nThink of it in three stages.\\r\\n\\r\\n \\r\\n\\r\\nFirst you prepare the ground.\\r\\n\\r\\n \\r\\n\\r\\nThen you lay the foundations.\\r\\n\\r\\n \\r\\n\\r\\nAnd then, finally, brick by brick, you build your house.\\r\\n\\r\\n \\r\\n\\r\\nPREPARING THE GROUND\\r\\n\\r\\n \\r\\n\\r\\nThese last ten months, we have been preparing the ground.\\r\\n\\r\\n \\r\\n\\r\\nOur Party\\x92s history tells us the ground on which political success is built.\\r\\n\\r\\n \\r\\n\\r\\nIt is the centre ground.\\r\\n\\r\\n \\r\\n\\r\\nNot the bog of political compromise.\\r\\n\\r\\n \\r\\n\\r\\nNot the ideological wilderness, out on the fringes of debate.\\r\\n\\r\\n \\r\\n\\r\\nBut the solid ground where people are.\\r\\n\\r\\n \\r\\n\\r\\nThe centre ground is where you find the concerns, the hopes and the dreams of most people and families in this country.\\r\\n\\r\\n \\r\\n\\r\\nIn 1979, they wanted a government to tame the unions, rescue our economy and restore Britain\\x92s pride.\\r\\n\\r\\n \\r\\n\\r\\nMargaret Thatcher offered precisely that alternative.\\r\\n\\r\\n \\r\\n\\r\\nAnd this Party can forever take pride in her magnificent achievements.\\r\\n\\r\\n \\r\\n\\r\\nToday, people want different things.\\r\\n\\r\\n \\r\\n\\r\\nThe priorities are different.\\r\\n\\r\\n \\r\\n\\r\\nSafer streets.\\r\\n\\r\\n \\r\\n\\r\\nSchools that teach.\\r\\n\\r\\n \\r\\n\\r\\nA better quality of life.\\r\\n\\r\\n \\r\\n\\r\\nBetter treatment for carers.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s what people are talking about today.\\r\\n\\r\\n \\r\\n\\r\\nBut for too long, we were having a different conversation.\\r\\n\\r\\n \\r\\n\\r\\nInstead of talking about the things that most people care about, we talked about what we cared about most.\\r\\n\\r\\n \\r\\n\\r\\nWhile parents worried about childcare, getting the kids to school, balancing work and family life - we were banging on about Europe.\\r\\n\\r\\n \\r\\n\\r\\nAs they worried about standards in thousands of secondary schools, we obsessed about a handful more grammar schools.\\r\\n\\r\\n \\r\\n\\r\\nAs rising expectations demanded a better NHS for everyone, we put our faith in opt-outs for a few.\\r\\n\\r\\n \\r\\n\\r\\nWhile people wanted, more than anything, stability and low mortgage rates, the first thing we talked about was tax cuts.\\r\\n\\r\\n \\r\\n\\r\\nFor years, this country wanted \\x96 desperately needed - a sensible centre-right party to sort things out in a sensible way.\\r\\n\\r\\n \\r\\n\\r\\nWell, that\\x92s what we are today.\\r\\n\\r\\n \\r\\n\\r\\nIn these past ten months we have moved back to the ground on which this Party\\x92s success has always been built.\\r\\n\\r\\n \\r\\n\\r\\nThe centre ground of British politics.\\r\\n\\r\\n \\r\\n\\r\\nAnd that is where we will stay.\\r\\n\\r\\n \\r\\n\\r\\nLAYING THE FOUNDATIONS \\x96 SOCIAL RESPONSIBILITY\\r\\n\\r\\n \\r\\n\\r\\nBut preparing the ground is just the first stage.\\r\\n\\r\\n \\r\\n\\r\\nNow we must show what we will build there.\\r\\n\\r\\n \\r\\n\\r\\nA strong government needs strong foundations.\\r\\n\\r\\n \\r\\n\\r\\nAnd I want us to lay those foundations this week.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s not about individual policies.\\r\\n\\r\\n \\r\\n\\r\\nIt is about a vision of the Britain we want to see.\\r\\n\\r\\n \\r\\n\\r\\nA Britain where we do not just ask what government can do.\\r\\n\\r\\n \\r\\n\\r\\nWe ask what people can do, what society can do.\\r\\n\\r\\n \\r\\n\\r\\nA Britain where we stop thinking you can pass laws to make people good.\\r\\n\\r\\n \\r\\n\\r\\nAnd start realising that we are all in this together.\\r\\n\\r\\n \\r\\n\\r\\nSocial responsibility \\x96 that is the essence of liberal Conservatism.\\r\\n\\r\\n \\r\\n\\r\\nThat is the idea I want us to explain this week.\\r\\n\\r\\n \\r\\n\\r\\nThat is what we stand for.\\r\\n\\r\\n \\r\\n\\r\\nThat is what we\\x92re fighting for.\\r\\n\\r\\n \\r\\n\\r\\nThat is the Britain we want to build.\\r\\n\\r\\n \\r\\n\\r\\nTake fighting crime.\\r\\n\\r\\n \\r\\n\\r\\nIt is not just a state responsibility.\\r\\n\\r\\n \\r\\n\\r\\nIt is a social responsibility.\\r\\n\\r\\n \\r\\n\\r\\nLet\\x92s not pretend that all we need is tough talk and tough laws to bring safety to our streets.\\r\\n\\r\\n \\r\\n\\r\\nOf course the state must play its part.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s why we\\x92re developing a programme of radical police reform.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s why we want to build more prisons and reform the ones we\\x92ve got, so they help reduce re-offending instead of encouraging it.\\r\\n\\r\\n \\r\\n\\r\\nAnd that\\x92s why we\\x92ll invest in drug rehabilitation, so we help addicts get clean and stay clean, instead of living a life of crime to feed their habit.\\r\\n\\r\\n \\r\\n\\r\\nBut that is not the end of the story.\\r\\n\\r\\n \\r\\n\\r\\nIt is just the start.\\r\\n\\r\\n \\r\\n\\r\\nWe need parents to bring up their children with the right values.\\r\\n\\r\\n \\r\\n\\r\\nWe need schools to be places of discipline and order.\\r\\n\\r\\n \\r\\n\\r\\nWe need to stand up for civilised values in public places.\\r\\n\\r\\n \\r\\n\\r\\nWe need to design crime out of the housing estates of the future.\\r\\n\\r\\n \\r\\n\\r\\nWe\\x92ve got to stop selling alcohol to children.\\r\\n\\r\\n \\r\\n\\r\\nWe need the music industry to understand that profiting from violent and homophobic words and images is morally wrong and socially unacceptable.\\r\\n\\r\\n \\r\\n\\r\\nBut more than this, we need people, families, communities, businesses to step up to the plate and understand that it\\x92s not just about stopping the bad things\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85it\\x92s about actively doing the good things.\\r\\n\\r\\n \\r\\n\\r\\nNot waiting for the state to do it all, but taking responsibility, making a difference, saying loudly and proudly: this is my country, this is my community: I will play my part.\\r\\n\\r\\n \\r\\n\\r\\nThat is social responsibility.\\r\\n\\r\\n \\r\\n\\r\\nThat is our idea.\\r\\n\\r\\n \\r\\n\\r\\nSo I want us to be the champions of a new spirit of social responsibility in this land.\\r\\n\\r\\n \\r\\n\\r\\nA new spirit of social responsibility that will succeed for Britain where Labour\\x92s outdated state responsibility has failed.\\r\\n\\r\\n \\r\\n\\r\\nLABOUR\\x92S APPROACH\\r\\n\\r\\n \\r\\n\\r\\nThink of any issue - not just crime - and then think of Labour\\x92s response.\\r\\n\\r\\n \\r\\n\\r\\nThis Government\\x92s way of doing things \\x96 the old way of doing things - is so familiar, and so depressing.\\r\\n\\r\\n \\r\\n\\r\\nMinisters hold a summit.\\r\\n\\r\\n \\r\\n\\r\\nThey announce an eye-catching initiative.\\r\\n\\r\\n \\r\\n\\r\\nA five-year plan.\\r\\n\\r\\n \\r\\n\\r\\nGordon Brown generously finds the money for it.\\r\\n\\r\\n \\r\\n\\r\\nThe money gets a headline, but no-one knows what to do with it.\\r\\n\\r\\n \\r\\n\\r\\nSo they create a unit in the Cabinet Office.\\r\\n\\r\\n \\r\\n\\r\\nA task force is set up.\\r\\n\\r\\n \\r\\n\\r\\nRegional co-ordinators are appointed.\\r\\n\\r\\n \\r\\n\\r\\nGordon Brown sets them targets \\x96 after all, it is his money.\\r\\n\\r\\n \\r\\n\\r\\nPilot schemes are launched.\\r\\n\\r\\n \\r\\n\\r\\nThe pilot schemes are rolled out across the country.\\r\\n\\r\\n \\r\\n\\r\\nThey are evaluated.\\r\\n\\r\\n \\r\\n\\r\\nThen revised, re-organised and re-launched.\\r\\n\\r\\n \\r\\n\\r\\nAnd then finally, once the reality dawns that the only people to benefit are the lawyers, accountants and consultants of Labour\\x92s quango army\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85with a pathetic whimper \\x96 but no hint of an apology \\x96 the whole thing is just abandoned.\\r\\n\\r\\n \\r\\n\\r\\nWe\\x92ve seen too much of this in the past nine years.\\r\\n\\r\\n \\r\\n\\r\\nHeadline after headline but absolutely no follow-through.\\r\\n\\r\\n \\r\\n\\r\\nIt is a story of ignorance, incompetence, arrogance.\\r\\n\\r\\n \\r\\n\\r\\nA story of wasted billions - and disappointed millions.\\r\\n\\r\\n \\r\\n\\r\\nSomewhere out there, there is a place where Blair and Brown will never go.\\r\\n\\r\\n \\r\\n\\r\\nIt\\x92s dark.\\r\\n\\r\\n \\r\\n\\r\\nIt\\x92s depressing.\\r\\n\\r\\n \\r\\n\\r\\nIt\\x92s haunted by the failures of nine years of centralisation, gimmick and spin.\\r\\n\\r\\n \\r\\n\\r\\nIt is the graveyard of initiatives, where you\\x92ll find the e-University that died a death,\\r\\n\\r\\n \\r\\n\\r\\nthe drugs czar that came and went\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85the Individual Learning Accounts that collapsed in fraud and waste, the tax credits that were paid and reclaimed\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85the Connexions service that flopped, the Strategic Health Authorities that were dropped\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85the marching of yobs to the hole in the wall; the night courts that never happened at all.\\r\\n\\r\\n \\r\\n\\r\\nAnd still they keep coming, those hubristic monuments to big government, the living dead that walk the well-trodden path from Downing Street and the Treasury to New Labour\\x92s graveyard of initiatives.\\r\\n\\r\\n \\r\\n\\r\\nThe NHS computer: delayed, disorganised, a £20 billion shambles.\\r\\n\\r\\n \\r\\n\\r\\nForced police mergers: the direct opposite of the community policing we need.\\r\\n\\r\\n \\r\\n\\r\\nAnd then the perfect example.\\r\\n\\r\\n \\r\\n\\r\\nID cards.\\r\\n\\r\\n \\r\\n\\r\\nWhen a half-way competent government would be protecting our security by controlling our borders\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85these Labour ministers are pressing ahead with their vast white elephant, their plastic poll tax, twenty Millennium Domes rolled into one giant catastrophe in the making.\\r\\n\\r\\n \\r\\n\\r\\nThey\\x92ve given up trying to find a good reason for it.\\r\\n\\r\\n \\r\\n\\r\\nLast week Tony Blair said that ID cards would help control immigration, when new immigrants won\\x92t even have them.\\r\\n\\r\\n \\r\\n\\r\\nDoes he even know what\\x92s going on in his Government?\\r\\n\\r\\n \\r\\n\\r\\nID cards are wrong, they\\x92re a waste of money, and we will abolish them.\\r\\n\\r\\n \\r\\n\\r\\nThese last nine years have been the story of a Government which instinctively believes, whatever it says, that everything is the state\\x92s responsibility.\\r\\n\\r\\n \\r\\n\\r\\nWe believe in social responsibility.\\r\\n\\r\\n \\r\\n\\r\\nBecause there is such a thing as society, it\\x92s just not the same thing as the state.\\r\\n\\r\\n \\r\\n\\r\\nTHE BRITAIN WE WANT TO SEE\\r\\n\\r\\n \\r\\n\\r\\nSo let us define this week the kind of Britain we want to see.\\r\\n\\r\\n \\r\\n\\r\\nAnd let us show how our idea \\x96 social responsibility\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85not Labour\\x92s idea \\x96 state responsibility\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85is the right response to the challenges Britain faces.\\r\\n\\r\\n \\r\\n\\r\\nGLOBALISATION, WELL-BEING, THE ENVIRONMENT\\r\\n\\r\\n \\r\\n\\r\\nWe know that in the age of globalisation, in the face of fast-moving economic change, people want their government to provide security.\\r\\n\\r\\n \\r\\n\\r\\nWe know that the end of the traditional 9 to 5 job can make life tough for families, and people look to their government for answers.\\r\\n\\r\\n \\r\\n\\r\\nAnd we know that in the race against time to tackle climate change and protect the environment, people expect their government to show leadership.\\r\\n\\r\\n \\r\\n\\r\\nOn all these challenges, Labour\\x92s first response is to regulate business, hoping to offer protection.\\r\\n\\r\\n \\r\\n\\r\\nIt may sound attractive.\\r\\n\\r\\n \\r\\n\\r\\nBut there are unintended consequences.\\r\\n\\r\\n \\r\\n\\r\\nWell-intentioned regulation can make us less secure in the age of globalisation.\\r\\n\\r\\n \\r\\n\\r\\nLess able to provide the jobs, wealth and opportunity on which well-being depends.\\r\\n\\r\\n \\r\\n\\r\\nIt can undermine the competitiveness of our companies, so it\\x92s harder for them to invest in the new, green technologies of the future.\\r\\n\\r\\n \\r\\n\\r\\nSo our response, based on our philosophy of social responsibility, is to say to business:\\r\\n\\r\\n \\r\\n\\r\\nYes you should look after your workers, yes you should look after your community, yes you should look after our environment.\\r\\n\\r\\n \\r\\n\\r\\nAnd we must stand up to big business when it\\x92s in the interests of Britain and the wider world.\\r\\n\\r\\n \\r\\n\\r\\nSo next week our MEPs will vote to strengthen proposals to make companies replace dangerous chemicals with safe ones.\\r\\n\\r\\n \\r\\n\\r\\nBut where Labour are casual about increasing regulation, we will be careful.\\r\\n\\r\\n \\r\\n\\r\\nWe will ask:\\r\\n\\r\\n \\r\\n\\r\\nAre we making it easier to start a business?\\r\\n\\r\\n \\r\\n\\r\\nEasier to employ someone?\\r\\n\\r\\n \\r\\n\\r\\nIs the overall burden of regulation going down?\\r\\n\\r\\n \\r\\n\\r\\nWill the regulation that\\x92s being put forward lead to real changes in behaviour, or just time-wasting and box-ticking?\\r\\n\\r\\n \\r\\n\\r\\nIf only we had a government that was asking these questions today.\\r\\n\\r\\n \\r\\n\\r\\nWe want companies to create their own solutions to social and environmental challenges, because those are the solutions most likely to last.\\r\\n\\r\\n \\r\\n\\r\\nSo in a Conservative Britain, corporate responsibility will provide the best long-term answer to economic insecurity, well-being in the workplace, and environmental care.\\r\\n\\r\\n \\r\\n\\r\\nIt is the same approach when you look at the other great challenges we face.\\r\\n\\r\\n \\r\\n\\r\\nPUBLIC SERVICES\\r\\n\\r\\n \\r\\n\\r\\nWe know that in an age of amazing technological advance, instant information exchange, and empowered consumers who don\\x92t have the deference of previous generations\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85people expect more from our health service and our schools.\\r\\n\\r\\n \\r\\n\\r\\nAnd government has to respond to that.\\r\\n\\r\\n \\r\\n\\r\\nLabour\\x92s response is the culture of targets, directives and central control, aimed at raising standards in our public services.\\r\\n\\r\\n \\r\\n\\r\\nThey mean well.\\r\\n\\r\\n \\r\\n\\r\\nBut the unintended consequence is to make these services less responsive to the people who use them, dashing expectations not meeting them.\\r\\n\\r\\n \\r\\n\\r\\nSo our response, based on our philosophy of social responsibility, is to say to our nurses, doctors, teachers:\\r\\n\\r\\n \\r\\n\\r\\nYes you should meet higher standards, yes you should give your patients and your pupils more.\\r\\n\\r\\n \\r\\n\\r\\nBut we\\x92re not going to tell you how to do it.\\r\\n\\r\\n \\r\\n\\r\\nYou are professionals.\\r\\n\\r\\n \\r\\n\\r\\nWe trust in your vocation\\r\\n\\r\\n \\r\\n\\r\\nSo in a Conservative Britain, professional responsibility will provide the answer to rising expectations in the NHS and schools.\\r\\n\\r\\n \\r\\n\\r\\nPOVERTY AND REGENERATION\\r\\n\\r\\n \\r\\n\\r\\nAnd just as people will no longer accept second best in public services, we know that in their communities they are fed up with squalor and poverty and crime\\x85\\r\\n\\r\\n \\r\\n\\r\\n\\x85and they look to their leaders to sort things out.\\r\\n\\r\\n \\r\\n\\r\\nLabour\\x92s response has been a massive expansion of central government into local communities.\\r\\n\\r\\n \\r\\n\\r\\nThe centralised Neighbourhood Renewal Unit, the insensitive Pathfinder programme, prescriptive top-down schemes for regeneration.\\r\\n\\r\\n \\r\\n\\r\\nYou can see why Labour have done it.\\r\\n\\r\\n \\r\\n\\r\\nBut the unintended consequence is to stifle the very spirit of community self-improvement that they are responding to.\\r\\n\\r\\n \\r\\n\\r\\nOur response, based on our philosophy of social responsibility, is to trust local leaders, not undermine them.\\r\\n\\r\\n \\r\\n\\r\\nSo we will hand power and control to local councils and local people who have the solutions to poverty, to crime, to urban decay in their hands.\\r\\n\\r\\n \\r\\n\\r\\nWe trust in your knowledge and commitment.\\r\\n\\r\\n \\r\\n\\r\\nSo in a Conservative Britain, civic responsibility will provide the answer to improving the quality of life in the communities left behind.\\r\\n\\r\\n \\r\\n\\r\\nCHILDREN\\r\\n\\r\\n \\r\\n\\r\\nAnd then perhaps the greatest challenge of all.\\r\\n\\r\\n \\r\\n\\r\\nThe challenge of bringing up children in a world that often seems fraught with risk and danger.\\r\\n\\r\\n \\r\\n\\r\\nThere is nothing that matters more to me than the safety and happiness of my family.\\r\\n\\r\\n \\r\\n\\r\\nOf course it\\x92s right that government should be on parents\\x92 side.\\r\\n\\r\\n \\r\\n\\r\\nBut Labour take it way too far.\\r\\n\\r\\n \\r\\n\\r\\nA national database of every child.\\r\\n\\r\\n \\r\\n\\r\\nMaking childcare a state monopoly.\\r\\n\\r\\n \\r\\n\\r\\nSlapping ASBOs on children who haven\\x92t even been born.\\r\\n\\r\\n \\r\\n\\r\\nLabour\\x92s intentions may be good.\\r\\n\\r\\n \\r\\n\\r\\nBut the unintended consequence is to create a culture of irresponsibility.\\r\\n\\r\\n \\r\\n\\r\\nThey may have abandoned Clause 4 and the nationalisation of industry.\\r\\n\\r\\n \\r\\n\\r\\nBut they are replacing it with the nationalisation of everyday life.\\r\\n\\r\\n \\r\\n\\r\\nThe state can never be everywhere, policing the interactions of our daily lives - and it shouldn\\x92t try.\\r\\n\\r\\n \\r\\n\\r\\nReal change will take years of patient hard work, and we will test every policy by asking: does it enhance parental responsibility?\\r\\n\\r\\n \\r\\n\\r\\nWe need to understand that cultural change is worth any number of government initiatives.\\r\\n\\r\\n \\r\\n\\r\\nWho has done more to improve school food, Jamie Oliver, or the Department of Education?\\r\\n\\r\\n \\r\\n\\r\\nPut another way, we need more of Supernanny, less of the nanny state.\\r\\n\\r\\n \\r\\n\\r\\nSo in a Conservative Britain, personal responsibility will provide the best answer to the risks and dangers of the modern world.\\r\\n\\r\\n \\r\\n\\r\\nPersonal responsibility.\\r\\n\\r\\n \\r\\n\\r\\nProfessional responsibility.\\r\\n\\r\\n \\r\\n\\r\\nCorporate responsibility.\\r\\n\\r\\n \\r\\n\\r\\nCivic responsibility.\\r\\n\\r\\n \\r\\n\\r\\nThese are the four pillars of our social responsibility.\\r\\n\\r\\n \\r\\n\\r\\nThat is the Britain we want to build.\\r\\n\\r\\n \\r\\n\\r\\nA Britain that is more green.\\r\\n\\r\\n \\r\\n\\r\\nMore family-friendly.\\r\\n\\r\\n \\r\\n\\r\\nMore local control over the things that matter.\\r\\n\\r\\n \\r\\n\\r\\nLess arrogant about politicians\\x92 ability to do it all on their own.\\r\\n\\r\\n \\r\\n\\r\\nBut more optimistic about what we can achieve if we all work together.\\r\\n\\r\\n \\r\\n\\r\\nWe want an opportunity society, not an overpowering state.\\r\\n\\r\\n \\r\\n\\r\\nBUILDING OUR HOUSE\\r\\n\\r\\n \\r\\n\\r\\nThis week, in our debates, we will lay the foundations of the house we are building together.\\r\\n\\r\\n \\r\\n\\r\\nThe foundations must come first.\\r\\n\\r\\n \\r\\n\\r\\nHow superficial, how insubstantial it would be, for us to make up policies to meet the pressures of the moment.\\r\\n\\r\\n \\r\\n\\r\\nPolicy without principle is like a house without foundations.\\r\\n\\r\\n \\r\\n\\r\\nIt will not stand the test of time.\\r\\n\\r\\n \\r\\n\\r\\nThat is what our Policy Review is all about: getting it right for the long term.\\r\\n\\r\\n \\r\\n\\r\\nOPTIMISM ABOUT BRITAIN\\x92S FUTURE\\r\\n\\r\\n \\r\\n\\r\\nIf we do this, we can help achieve so much for this country.\\r\\n\\r\\n \\r\\n\\r\\nIn a few years\\x92 time, Britain could wake up to a bright new morning.\\r\\n\\r\\n \\r\\n\\r\\nWe have everything to be optimistic about.\\r\\n\\r\\n \\r\\n\\r\\nYou could not design a country with better natural advantages than we have.\\r\\n\\r\\n \\r\\n\\r\\nWe speak the language of the world.\\r\\n\\r\\n \\r\\n\\r\\nWe have links of history and culture with every continent on earth.\\r\\n\\r\\n \\r\\n\\r\\nWe have institutions \\x96 our legal system, our armed forces, the BBC, our great universities \\x96 which set the standard that all other countries measure themselves by.\\r\\n\\r\\n \\r\\n\\r\\nOur artists, writers and musicians inspire people the world over.\\r\\n\\r\\n \\r\\n\\r\\nWe are inventive, creative, irreverent and daring.\\r\\n\\r\\n \\r\\n\\r\\nIn this young century, these old advantages give us the edge we need.\\r\\n\\r\\n \\r\\n\\r\\nCONCLUSION\\r\\n\\r\\n \\r\\n\\r\\nWhat a prospect for a great Party \\x96 to guide our nation at this time of opportunity.\\r\\n\\r\\n \\r\\n\\r\\nSo let us stick to the plan.\\r\\n\\r\\n \\r\\n\\r\\nLet us build - carefully, thoughtfully and patiently, a new house together.\\r\\n\\r\\n \\r\\n\\r\\nPreparing the ground as we move to the centre, meeting the priorities of the modern world.\\r\\n\\r\\n \\r\\n\\r\\nLaying the foundations with our idea - social responsibility.\\r\\n\\r\\n \\r\\n\\r\\nAnd building on those foundations with the right policies for our long-term future.\\r\\n\\r\\n \\r\\n\\r\\nThe nation\\x92s hopes are in our hands.\\r\\n\\r\\n \\r\\n\\r\\nPeople\\x92s hopes.\\r\\n\\r\\n \\r\\n\\r\\nYour hopes.\\r\\n\\r\\n \\r\\n\\r\\nMy hopes.\\r\\n\\r\\n \\r\\n\\r\\nIn eight days\\x92 time I will be forty years old.\\r\\n\\r\\n \\r\\n\\r\\nI have so much to look forward to.\\r\\n\\r\\n \\r\\n\\r\\nMy young family.\\r\\n\\r\\n \\r\\n\\r\\nThey have so much to look forward to.\\r\\n\\r\\n \\r\\n\\r\\nThe world I want for them is the world I want for every family and every community.\\r\\n\\r\\n \\r\\n\\r\\nIf you want to know what I\\x92m all about, I can explain it one word.\\r\\n\\r\\n \\r\\n\\r\\nThat word is optimism.\\r\\n\\r\\n \\r\\n\\r\\nI am optimistic about human nature.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s why I will trust people to do the right thing.\\r\\n\\r\\n \\r\\n\\r\\nLabour are pessimists.\\r\\n\\r\\n \\r\\n\\r\\nThey think that without their guidance, people will do the wrong thing.\\r\\n\\r\\n \\r\\n\\r\\nThat\\x92s why they want to regulate and control.\\r\\n\\r\\n \\r\\n\\r\\nSo let us show clearly which side we are on.\\r\\n\\r\\n \\r\\n\\r\\nLet optimism beat pessimism.\\r\\n\\r\\n \\r\\n\\r\\nLet sunshine win the day.\\r\\n\\r\\n \\r\\n\\r\\nAnd let everyone know that the Conservative Party is ready.\\r\\n\\r\\n \\r\\n\\r\\nReady to serve.\\r\\n\\r\\n \\r\\n\\r\\nReady to fight.\\r\\n\\r\\n \\r\\n\\r\\nReady to win.'"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html",
    "href": "notebooks/W05. Merging and Joining.html",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "",
    "text": "Sometimes, we will want to combine data from different sources about the same subject - perhaps we want to compare the GDP in a country with life expectancy, or the proportion of free schools meals with the level of unemployment.\n\n\n\nUnderstand joins\nWork with joining dataframes in Pandas\nCreate your own examples"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#downloading-the-data",
    "href": "notebooks/W05. Merging and Joining.html#downloading-the-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Downloading the Data",
    "text": "Downloading the Data\nLet’s grab the data we will need this week from our course website and save it into our data folder. If you’ve not already created a data folder then do so using the following command.\nDon’t worry if it generates an error, that means you’ve already got a data folder.\n\n!mkdir data\n\n\n!mkdir data/wk3\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk3/UN_Life_all.csv -o ./data/wk3/UN_Life_all.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk3/UN_Cities_1214_country.csv -o ./data/wk3/UN_Cities_1214_country.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk3/UN_Cities_1214_population.csv -o ./data/wk3/UN_Cities_1214_population.csv"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#joining-instructions",
    "href": "notebooks/W05. Merging and Joining.html#joining-instructions",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Joining Instructions",
    "text": "Joining Instructions\nJoins are the combination of different datasets, and are common in relational databases as a way of performing queries. There are lots of examples of why and when we might want to do this, but most start with two tables of data. We’re going to start with some data we’ve generated.\nI’m going to go back and work with fake data for a while, because it’s clean and small and we can see what’s going on - when we work with real data, we have to take great care that the data is clean, the indices match, and so on.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n%matplotlib inline\nplt.style.use('ggplot')\n\nLet’s create dataframes which represent fictitious values associated with people. Let’s assume our data is anonymised because we’re ethical researchers and don’t want information about real people leaking out.\n\npeople1 = pd.DataFrame(5+np.random.randn(5, 5))\npeople1.columns = ['units of alcohol drunk','cigarettes smoked','sleep per night','height','BMI']\n\n\npeople1\n\n\npeople2 = pd.DataFrame(5+np.random.randn(3, 5))\npeople2.columns = ['units of alcohol drunk','cigarettes smoked','sleep per night','height','BMI']\n\n\npeople2"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#data-with-a-unique-index-adding-new-observations",
    "href": "notebooks/W05. Merging and Joining.html#data-with-a-unique-index-adding-new-observations",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Data with a unique index: adding new observations",
    "text": "Data with a unique index: adding new observations\nLet’s now examine data where the elements of study are not anonymous. Let’s consider that we have some city data. If we have city names (or equivalent) in the index column, simply concatenating them would be fine, because the names would not repeat in the way the index has above.\n\ndf1 = pd.DataFrame(5+np.random.randn(5, 5))\ndf1.columns = ['area','population','mean temperature','elevation','annual rainfall']\ndf1.index = ['London', 'Paris', 'Beijing', 'Medellin', 'Port Elizabeth']\n\n\ndf1\n\n\ndf2 = pd.DataFrame(5+np.random.randn(3, 5))\ndf2.columns = ['area','population','mean temperature','elevation','annual rainfall']\ndf2.index = ['Mumbai', 'Sydney', 'Boston']\n\n\ndf2\n\n\ndf3 = pd.concat([df1,df2])\n\n\ndf3"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#exercise-concat-continued",
    "href": "notebooks/W05. Merging and Joining.html#exercise-concat-continued",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise: Concat continued",
    "text": "Exercise: Concat continued\nRepeat the above for fictitious values for New York, Tokyo, Manila and Budapest - concatenate into a new dataframe “df”."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#combining-on-attributes",
    "href": "notebooks/W05. Merging and Joining.html#combining-on-attributes",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Combining on Attributes",
    "text": "Combining on Attributes\nWhat if we’re looking at the same locations but different attributes? Consider the same df1\n\ndf1 = pd.DataFrame(5+np.random.randn(5, 5))\ndf1.columns = ['area','population','mean temperature','elevation','annual rainfall']\ndf1.index = ['London', 'Paris', 'Beijing', 'Medellin', 'Port Elizabeth']\n\n\ndf1\n\nBut a new dataframe df4, which details the same locations, but has different information about them:\n\ndf4 = pd.DataFrame(5+np.random.randn(5, 3))\ndf4.columns = ['Mean House Price', 'median income','walkability score']\ndf4.index = ['London', 'Paris', 'Beijing', 'Medellin', 'Port Elizabeth']\n\n\ndf4\n\nWe have to join “on” the index - meaning when merging the records, python will look at the index column.\n\ndf_joined = df1.merge(df4, left_index=True, right_index=True)\n\n\ndf_joined\n\nNote that this joins on the index, not the row number - so if the order of elements in df4 is different, it should still work.\n\ndf4 = pd.DataFrame(np.random.randn(5, 3))\ndf4.columns = ['Mean House Price', 'median income','walkability score']\ndf4.index = ['Paris','Port Elizabeth', 'Beijing', 'Medellin', 'London']\n\n\ndf1\n\n\ndf4\n\n\ndf_joined = df1.merge(df4, left_index=True, right_index=True)\n\n\ndf_joined"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#merge-records",
    "href": "notebooks/W05. Merging and Joining.html#merge-records",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Merge Records",
    "text": "Merge Records\nConsider now a case where we have data for some but not all cities; so df1 stil has data for these 5 cities:\n\ndf1\n\nBut our new table, df5, contains data for three cities:\n\ndf5 = pd.DataFrame(5+np.random.randn(3, 3))\ndf5.columns = ['Mean House Price', 'median income','walkability score']\ndf5.index = ['London', 'Paris', 'Glasgow']\n\n\ndf5"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#exercise",
    "href": "notebooks/W05. Merging and Joining.html#exercise",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise:",
    "text": "Exercise:\nHow many cities appear in: - both dataframes - only df1 - only df5 - neither df1 nor df5?"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#way-back-venn",
    "href": "notebooks/W05. Merging and Joining.html#way-back-venn",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Way Back Venn",
    "text": "Way Back Venn\nWhat is the mechanism for joining data where these mismatches exist? Well, there are several, starting with the…"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#inner-join",
    "href": "notebooks/W05. Merging and Joining.html#inner-join",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Inner Join:",
    "text": "Inner Join:\n\nfrom IPython.display import Image\n\ndata_path = \"https://s3.eu-west-2.amazonaws.com/qm2/wk3/inner.png\"\nImage(data_path)\n\n(Image from http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/)\nThe inner join only includes data whose index appears in both tables. Let’s see what that looks like:\n\ndf_joined = df1.merge(df5, left_index=True, right_index=True)\n\n\ndf_joined\n\nHere, we have a couple of arguments specifying the manner of the join - we have specified that we are joining on the index of the left and right dataset with the optional “left_index=True” and “right_index=True”. Less obviously, the left dataset is df1 (because we’re using df1.merge() and the right dataset is df5 (because it appears as an argument in merge(). There’s no special reason it shouldn’t be the other way around, but for this function, it is this way around and we need to remember that when we use it."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#inner-space",
    "href": "notebooks/W05. Merging and Joining.html#inner-space",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Inner Space",
    "text": "Inner Space\nAlthough we haven’t specified it, the merge() function has defaulted to an inner join (like the diagram above). We can specify how the join is calculated by changing the text in the optional argument “how”:\n\ndf_joined = df1.merge(df5, left_index=True, right_index=True, how='inner')\n\n\ndf_joined"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#the-future-of-the-left",
    "href": "notebooks/W05. Merging and Joining.html#the-future-of-the-left",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "The Future of The Left",
    "text": "The Future of The Left\nThe left join includes all rows where the index appears on the left hand side of the join, and any data which matches it on the right hand side. If the index appears on the left but not the right, it will include the data from the left table, and have blanks for the columns on the right.\n\ndata_path = \"https://s3.eu-west-2.amazonaws.com/qm2/wk3/left.png\"\nImage(data_path)\n\nWhat does this look like? We will use the how=‘left’ optional argument to create a left join:\n\ndf_joined = df1.merge(df5, left_index=True, right_index=True, how='left')\n\n\ndf_joined\n\nAs we see, the missing data appears as NaN - Not a Number."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#exercise-1",
    "href": "notebooks/W05. Merging and Joining.html#exercise-1",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise:",
    "text": "Exercise:\nCarry out right and outer joins on the dataframes df1 and df5 and explain how they’re filtering and joining the data."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#i-am-the-one-and-only",
    "href": "notebooks/W05. Merging and Joining.html#i-am-the-one-and-only",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "I Am The One and Only",
    "text": "I Am The One and Only\nSo far, we’ve carried out joins on data which have a one-to-one relationship; data for cities or people. What if our data has a one-to-many correspondence?\nExample: We want to look at the quality of life in cities (a real student project from 2014). We have a dataset listing city-level characteristics for a number of cities in Europe, including the country each city is in. We also have a dataset listing the GDP, life expectancy and other indicators for a number of countries in Europe. How do we create a dataframe which, for each city, lists all of the characteristics of a city and those of its parent country?\nWe’ll be working now with data from the UN, covering information about cities - real data this time. The UN has some great data, we’ve taken some from here and processed it in various ways:\nhttp://data.un.org/Data.aspx?d=POP&f=tableCode%3A240\nLet’s load up data on city population - this set contains data for 2012-2014 inclusive:\n\ndata_path = \"./data/wk3/UN_Cities_1214_population.csv\"\n\ncity_pop = pd.read_csv(data_path, encoding='latin1')\n\n\ncity_pop.head()"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#exercise-2",
    "href": "notebooks/W05. Merging and Joining.html#exercise-2",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nThere is a another datafile we downloaded called UN_Cities_1214_country.csv. This is saved to ./data/wk3/UN_Cities_1214_country.csv - Load this into a dataframe called city_c with the city name as the index and view it; then, using merge on city name with city_pop to create a new dataframe called cities.\nHints: You’ll notice that the index won’t be the column you want to merge on in the city_pop data. What column should you merge on in city_pop? Which column should you merge on in city_c?\nThe syntax for merging on a column (which is not the index) is to pass the column name to the optional ‘left_on=’ or ‘right_on=’ arguments. And we don’t use right_index=True (or left_index=True), depending on which we’re using.\nSo for example: df1.merge(df2, left_on=‘Name’, right_index=True) would join df1 (on the left) to df2 (on the right), using the column ‘Name’ on the left (df1) and the index column (whatever that is) on the right (df2)."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#a-footnote-about-footnotes",
    "href": "notebooks/W05. Merging and Joining.html#a-footnote-about-footnotes",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "A footnote about footnotes",
    "text": "A footnote about footnotes\nJust a quick note - if you look at the primary UN data, you’ll see footnotes which will confuse the hell out of Pandas. I’ve taken the footnotes out, but you can use .tail() to see whether there’s any junk in the trunk, and remove it via a text editor."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#clean-data",
    "href": "notebooks/W05. Merging and Joining.html#clean-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Clean data",
    "text": "Clean data\nWe need to simplify this data a bit in the following ways:\n\nI’m going to focus on one year (2012)\nI’m going to just look at “Both Sexes” (not focus on one gender)\nI’m going to get rid of a column of data (the ‘Value Footnotes’ column) using the drop() method.\n\n\ncities = cities[cities['Sex']=='Both Sexes']\ncities = cities[cities['Year']==2012]\ncities.drop('Value Footnotes', axis=1, inplace=True)\n\n\ncities.head()"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#extension-in-my-place",
    "href": "notebooks/W05. Merging and Joining.html#extension-in-my-place",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Extension: In My Place",
    "text": "Extension: In My Place\nThe command I used to get rid of that column is cities.drop(‘Value Footnotes’, axis=1, inplace=True). The syntax is not so complex - the first argument, ‘Value Footnotes’, is just the name of the column; the second argument, axis=1, tells Pandas to look for a column to remove (instead of a row which has axis=0); the third and final argument, inplace=True, is a command that tells Pandas to edit inplace, i.e. to edit the dataframe (cities) directly. When inplace is False (the default), this command does not directly edit cities, but instead provide an output. So the syntax for that would be\nnew_cities = cities.drop(‘Value Footnotes’, axis=1)\nand new_cities would be a version of cities without the offending column. This is usually the safer option."
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#life-oh-life",
    "href": "notebooks/W05. Merging and Joining.html#life-oh-life",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Life, Oh Life",
    "text": "Life, Oh Life\nThe UN also has useful data by country, so let’s try and work with some of that and join it up with our city data. Let’s work with Life Expectancy Data:\nhttp://data.un.org/Data.aspx?d=WDI&f=Indicator_Code%3ASP.DYN.LE00.IN\n\ndata_path = \"./data/wk3/UN_Life_all.csv\"\nlife = pd.read_csv(data_path, index_col=0)\n\n\nlife.head()"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#exercise-3",
    "href": "notebooks/W05. Merging and Joining.html#exercise-3",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise:",
    "text": "Exercise:\nIn a new cell, clean up the above dataframe by\n\nremoving the “Value Footnotes” Column\nuse only the most recent data (2012)\n\nLet’s make it a little clearer what “Value” refers to, by renaming the column. This is one way to do that:\n\nlife.rename(columns={'Value':'Life Expectancy'}, inplace=True)\n\n\nlife.head()"
  },
  {
    "objectID": "notebooks/W05. Merging and Joining.html#recap-joins",
    "href": "notebooks/W05. Merging and Joining.html#recap-joins",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Recap: Joins",
    "text": "Recap: Joins\nPandas has four join methods:\n- Left Join: use **only** keys from **left** DataFrame. SQL: [left outer join](http://goo.gl/JICveI)\n- Right Join: use **only** keys from **right** DataFrame. SQL: [right outer join](http://goo.gl/TrrHjQ)\n- Outer Join: use union of **keys from both** DataFrames. SQL: [full outer join](http://goo.gl/bVRqO8)\n- Inner Join: use **intersection of keys** from both DataFrames. SQL: [inner join](http://goo.gl/Cf1MF8)"
  },
  {
    "objectID": "Week 6.html",
    "href": "Week 6.html",
    "title": "6  Week 1",
    "section": "",
    "text": "This is an embedded <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> presentation, powered by <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html",
    "href": "notebooks/W07. Distributions and Basic Statistics.html",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "",
    "text": "For the rest of this course, we’ll be working with data from the U.S. Census [Current Population Survey] (https://www.census.gov/programs-surveys/cps/technical-documentation/methodology.html)."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#alias-grace",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#alias-grace",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Alias Grace",
    "text": "Alias Grace\nThe first thing to notice is that I’m using an alias for matplotlib.pyplot - it’s a bit ungainly, so I’m using “plt” in its place. That’s just to make the coding easier. I’ll do the same for some of the other libraries as we go through - this isn’t necessary, but online examples frequently use “pd” for “pandas” (for example), so it can be useful to use these. The way it works is pretty simple - now I’ve used “plt” as my alias for matplotlib.pyplot, I can just say “plt.command()” whenever I need to use functions from that library.\nOk, so in this next block I’m going to generate some synthetic data which will be called “heights”:\n\nbins = 100\nsample = 10000\nmean = 163\nsd = 6\n\nheights = sd * numpy.random.randn(sample) + mean\n\n\n#now we plot this fake data\nplt.hist(heights)\nplt.xlabel('Height [cm]')\nplt.ylabel('Frequency')\nplt.title('Fake Height Data')\n\nText(0.5, 1.0, 'Fake Height Data')"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#at-peace-with-arguments",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#at-peace-with-arguments",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "At peace with arguments",
    "text": "At peace with arguments\nThese functions (for drawing graphs, adding axes, etc) take arguments. For example, xlabel() “takes” a text value which tells it what to write - this is a required argument. hist() takes required arguments like height, and optional arguments like bins (how many bins to use) and histtype (how the graph appears)."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#exercise-changing-arguments",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#exercise-changing-arguments",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Exercise: changing arguments",
    "text": "Exercise: changing arguments\nWhat effect does removing these arguments have? What if you change their values? Use the hist() documentation page to understand what other effects you can create.\nNote that I’ve not used real data, I’ve just sampled randomly from a curve which represents the spread of heights - so the x axis is height, and the y axis is how frequent, or equivalently how likely, that height is to occur. In the real world, more often than not, you will have to figure out what this curve is.\nImportant Side Note: Always, always label your data and axes\nOk, what is this curve I’ve used?"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#th-moment",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#th-moment",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "0th moment",
    "text": "0th moment\n0th moment is just area under the curve. In this case it would just be the number of people sampled. It doesn’t tell us much."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#st-moment",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#st-moment",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "1st moment",
    "text": "1st moment\n1st moment is a central “tendency” or average value of the distribution. In a Gaussian, the mean (average) \\(\\mu\\) is the average and the centre. But there are other ways of representing this characteristic, like mode or median, as we will see."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#nd-moment",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#nd-moment",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "2nd moment",
    "text": "2nd moment\nSecond moment represents the width or spread of the curve. \\(\\sigma\\) is calculated from the second moment, and in a Gaussian represents the width of the curve. Interquartile range is another way of doing this, or half-life."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#rd-moment",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#rd-moment",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "3rd moment",
    "text": "3rd moment\nThe third moment is related to how asymmetrical a curve is, and the usual way to quantify this is skewness. A Gaussian is symmetric, so its skewness is zero. Actually, all moments apart from 0, 1 and 2 are zero for a Gaussian."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#th-moment-1",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#th-moment-1",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "4th moment",
    "text": "4th moment\nThe standard way to calculate a fourth moment is Kurtosis. This is a measure of how “peaky” or “taily” a curve is. It’s a bit harder to visualise what we’re on about at this point, but it’s worth remembering that a Gaussian has a Kurtosis of 0, so isn’t very “taily”."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#other-moments",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#other-moments",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Other moments",
    "text": "Other moments\nIt’s harder to interpret what they mean, but an arbitrary probability distribution may need an infinite series of moments to be described correctly. Gaussians are unusual in that they require only two parameters to describe them; if your data looks Gaussian, that makes life easier in many ways.\nExtension: there are distributions which are simple to write down that nevertheless are described by an infinite series of moments; and some, like the Lorentzian, are quite nice functions that have infinite values for most of their moments."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#recap-the-median",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#recap-the-median",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Recap: The Median",
    "text": "Recap: The Median\nThe median is different from the mean. The median of a data set is an element that seperates the higher half of the data from the lower half. For example, the median of \\(A = \\left \\{1,2,3,4,5\\right \\}\\) is 3, and 3 is also the median of \\(B = \\left \\{1,2,3,9,10\\right \\}\\). When a data set has an even number of members like \\(C = \\left \\{1,2,3,4\\right \\}\\), the median is defined as the average of the top lower half and the lower top half, so the median of C is (2+3)/2=2.5 .\nAdvanced: How did we define the median from a distribution (hint: consider area under curve and what it means)? What about quartiles? What about deciles? What are these quantities?"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#exercise",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#exercise",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Exercise",
    "text": "Exercise\nUnder which conditions is the median equal to the mean? (hint: look at the normal distribution - but think more broadly abouting how “counting” and “area under curve” are related in histograms)."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#exercise-1",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#exercise-1",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Exercise",
    "text": "Exercise\nConstruct a set of 5 numbers such that the median is half the value of the mean."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#power-laws",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#power-laws",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Power Laws",
    "text": "Power Laws\nAnother type of important distributions are power law distributions. When the probability of a variable to take the value of \\(x\\) is roughly \\(\\frac{1}{x^k}\\) we say that \\(x\\) has a power law distribution with exponent \\(k\\).\nFor instance let’s plot a power law function with exponent 3.\nWe start off by generating an array of 100 numbers between 1 and 10; numpy.linspace() lets us do this (here is the documentation if you want to find out more: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n\nx_val= numpy.linspace(1,10,100)\n#** is \"to the power of\" in Pythonese\ny_val = 1/x_val**3\nplt.plot(x_val,y_val)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.xlim(1,5)\n\n(1.0, 5.0)\n\n\n\n\n\nAn important class of power law distributions are called Pareto type distributions. Consider for instance, the distribution of income in a city. We will find that about 20% of the population has 80% of the wealth, this is also known as a 80:20 law.\n\nsample = 10000\nexponent = 3\n\nbins = 1000\n\n\ny = 1 + numpy.random.pareto(exponent, sample)\n\nplt.hist(y, bins, histtype='step')\nplt.xlim((1, 5))\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#exercise-2",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#exercise-2",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Exercise:",
    "text": "Exercise:\nAdd lines showing the mean, mode and median to the above graph, and comment on this with a box of markdown."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#comments",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#comments",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Comments:",
    "text": "Comments:\nWe plotted the mean, mode and median of the distribution…"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#extension-quantifying-datasets-which-cover-a-wide-range-of-values.",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#extension-quantifying-datasets-which-cover-a-wide-range-of-values.",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Extension: Quantifying Datasets Which Cover a Wide Range of Values.",
    "text": "Extension: Quantifying Datasets Which Cover a Wide Range of Values.\nObjects like the mean present a problem for power laws - in some cases, they diverge. So, for the mathematicians, if the exponent is 1, we can approximate the mean by\n\\(\\mu = \\int_0^{\\infty}dx \\frac{1}{x}*x = \\int_0^{\\infty}dx = \\infty\\)!\nwhich gives a nonsensical value. Larger exponents (larger than 2) ensure this doesn’t occur - but what sense does the mean make when there are people earning 10, 100 or 1000 times that mean value?\nWhen datasets cover many orders of magnitude (here from 33 to 2090), we may need to think about geometrical methods for understanding its properties. For the mean, we might consider the geometrical mean, achieved by mutiplying the 100 values together and taking the 100th root. To understand the width, we might use a half-life type measure. Half-life is the “time” it takes for a radioactive source to reach half its current level of radioactivity; but we can think about applying this to other situations, where “time” is replaced by “percentile” and “radioactivity” is replaced by “income”, for example."
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#extension-bimodal-distributions",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#extension-bimodal-distributions",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Extension: Bimodal Distributions",
    "text": "Extension: Bimodal Distributions\nSometimes distributions are Bimodal or multimodal. What this literally means is that they have multiple “modes” - most popular values - i.e. peaks. In these cases, mean may not be the best measure and we may need to think about these two (or larger number of) peaks relates to different segments of our sample or population.\n\nsample = 100000\na = 25 * numpy.random.randn(sample) + 20\nfor i in range(int(sample/2)):\n    a[i] = a[i]*1.3+100\nplt.hist(a, 250, histtype='step');\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')"
  },
  {
    "objectID": "notebooks/W07. Distributions and Basic Statistics.html#exercise-3",
    "href": "notebooks/W07. Distributions and Basic Statistics.html#exercise-3",
    "title": "Workshop 5: Working with Data and Basic Statistics",
    "section": "Exercise:",
    "text": "Exercise:\nPlot the mean and median for the above graph. What might be better statistics? How might you find them?"
  },
  {
    "objectID": "Week 8.html",
    "href": "Week 8.html",
    "title": "8  Week 1",
    "section": "",
    "text": "This is an embedded <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> presentation, powered by <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "notebooks/W09. Machine Learning.html",
    "href": "notebooks/W09. Machine Learning.html",
    "title": "Set up TensorFlow",
    "section": "",
    "text": "This short introduction uses Keras to:\nImport TensorFlow into your program to get started:\nIf you are following along in your own development environment, rather than Colab, see the install guide for setting up TensorFlow for development.\nNote: Make sure you have upgraded to the latest pip to install the TensorFlow 2 package if you are using your own development environment. See the install guide for details."
  },
  {
    "objectID": "notebooks/W09. Machine Learning.html#load-a-dataset",
    "href": "notebooks/W09. Machine Learning.html#load-a-dataset",
    "title": "Set up TensorFlow",
    "section": "Load a dataset",
    "text": "Load a dataset\nLoad and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers:\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\n\n\n\n\ndef plot_num(number):\n\n  item_index = np.where(y_train[:1000]==number)\n  subset=x_train[item_index]\n  \n  egs=5\n  fig, axs = plt.subplots(1,egs, figsize=(20,10))\n\n  for i in range(0,egs):\n    axs[i].imshow(subset[i])\n\n\nfor x in range(0,10):\n  plot_num(x)"
  },
  {
    "objectID": "notebooks/W09. Machine Learning.html#build-a-machine-learning-model",
    "href": "notebooks/W09. Machine Learning.html#build-a-machine-learning-model",
    "title": "Set up TensorFlow",
    "section": "Build a machine learning model",
    "text": "Build a machine learning model\nBuild a tf.keras.Sequential model by stacking layers.\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\n\nFor each example, the model returns a vector of logits or log-odds scores, one for each class.\n\npredictions = model(x_train[:1]).numpy()\npredictions\n\narray([[ 0.33447966,  0.35151538, -0.35632688, -0.2735348 ,  0.8568236 ,\n        -0.8503612 , -0.34408805, -0.40424255,  0.4791569 , -0.00873779]],\n      dtype=float32)\n\n\nThe tf.nn.softmax function converts these logits to probabilities for each class:\n\ntf.nn.softmax(predictions).numpy()\n\narray([[0.12650587, 0.12867945, 0.06340117, 0.0688737 , 0.21328573,\n        0.03868484, 0.06418189, 0.06043489, 0.1461986 , 0.08975388]],\n      dtype=float32)\n\n\nNote: It is possible to bake the tf.nn.softmax function into the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it’s impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.\nDefine a loss function for training using losses.SparseCategoricalCrossentropy, which takes a vector of logits and a True index and returns a scalar loss for each example.\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nThis loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\nThis untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\n\nloss_fn(y_train[:1], predictions).numpy()\n\n3.2523074\n\n\nBefore you start training, configure and compile the model using Keras Model.compile. Set the optimizer class to adam, set the loss to the loss_fn function you defined earlier, and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy.\n\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])"
  },
  {
    "objectID": "notebooks/W09. Machine Learning.html#train-and-evaluate-your-model",
    "href": "notebooks/W09. Machine Learning.html#train-and-evaluate-your-model",
    "title": "Set up TensorFlow",
    "section": "Train and evaluate your model",
    "text": "Train and evaluate your model\nUse the Model.fit method to adjust your model parameters and minimize the loss:\n\nmodel.fit(x_train, y_train, epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.2955 - accuracy: 0.9139\nEpoch 2/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1416 - accuracy: 0.9578\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1072 - accuracy: 0.9676\nEpoch 4/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0874 - accuracy: 0.9727\nEpoch 5/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0734 - accuracy: 0.9772\nEpoch 6/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0632 - accuracy: 0.9792\nEpoch 7/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0575 - accuracy: 0.9808\nEpoch 8/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0526 - accuracy: 0.9825\nEpoch 9/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0460 - accuracy: 0.9853\nEpoch 10/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0437 - accuracy: 0.9858\n\n\n<keras.callbacks.History at 0x7fe943384b10>\n\n\nThe Model.evaluate method checks the models performance, usually on a “Validation-set” or “Test-set”.\n\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n313/313 - 1s - loss: 0.0715 - accuracy: 0.9785 - 529ms/epoch - 2ms/step\n\n\n[0.07152838259935379, 0.9785000085830688]\n\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the TensorFlow tutorials.\nIf you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\n\n\n#probability_model(x_test[:1])\npredictions=probability_model.predict(x_test)\n\nindex=20\n\nprint(np.argmax(predictions[index]))\nplt.imshow(x_test[index])\n\n9\n\n\n<matplotlib.image.AxesImage at 0x7fe9409c1ad0>"
  },
  {
    "objectID": "notebooks/W09. Machine Learning.html#conclusion",
    "href": "notebooks/W09. Machine Learning.html#conclusion",
    "title": "Set up TensorFlow",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the Keras API.\nFor more examples of using Keras, check out the tutorials. To learn more about building models with Keras, read the guides. If you want learn more about loading and preparing data, see the tutorials on image data loading or CSV data loading."
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "",
    "text": "We’ve successfully plotted spatial data using the core pandas and matplotlib libraries. In this session, we will use geopandas - a library which is designed explicitly to work with apatial data; it does projection and allows us to plot geometries and create choropleth maps.\n\n\n\nCreate and view a geopandas dataframe\nUnderstand the significance of the geometry column\nThe basics of projection\nCreate a choropleth map, based on data\nJoin dataframes to match data to geographies\nPlot point data (extension)\nPlot point data with a basemap (extension)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#install-packages",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#install-packages",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Install packages",
    "text": "Install packages\nFirstly, we will need to install Geopandas and pysal so that we can use it within this workshop. This will generate alot of text and might take a minute to complete. This is how we install packages within Azure Notebooks.\n\n#Install newest branch\n!pip install pysal\n\n#Install the geopandas module\n!pip install geopandas"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#downloading-the-data",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#downloading-the-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Downloading the Data",
    "text": "Downloading the Data\nLet’s grab the data we will need this week from our course website and save it into our data folder. If you’ve not already created a data folder then do so using the following command.\nDon’t worry if it generates an error, that means you’ve already got a data folder.\n\n!mkdir data\n\n\n!mkdir data/wk9\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/london_wards.shp -o ./data/wk9/london_wards.shp\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/london_wards.cpg -o ./data/wk9/london_wards.cpg\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/london_wards.dbf -o ./data/wk9/london_wards.dbf\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/london_wards.prj -o ./data/wk9/london_wards.prj\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/london_wards.shx -o ./data/wk9/london_wards.shx\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/2011persons.csv -o ./data/wk9/2011persons.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/borough_density.csv -o ./data/wk9/borough_density.csv\n!curl https://s3.eu-west-2.amazonaws.com/qm2/wk9/tweet_data.csv -o ./data/wk9/tweet_data.csv"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#setup-our-enviroment",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#setup-our-enviroment",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Setup our enviroment",
    "text": "Setup our enviroment\n\n!pip install descartes\n!pip install mapclassify\n\n\nfrom pysal import *\nimport geopandas as gp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pylab\nimport descartes\nimport mapclassify\n\n%matplotlib inline\n\nplt.style.use('ggplot')\npylab.rcParams['figure.figsize'] = (20., 16.)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#the-shape-of-things-to-come",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#the-shape-of-things-to-come",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "The shape of things to come",
    "text": "The shape of things to come\nGeopandas works with shapefiles - an industry standard in spatial analysis and GIS created by ESRI, who make a lot of GIS products. Shapefiles actually involves a number of files (filename.shp, filename.shx, filename.dbf, filename.proj and filename.cpg) which encode the polygons, data, projection and various other useful bits of data. Suffice it to say that we generally need all of these bits, with the same filename (but different extension), in the same folder. Generally, if you download shapefiles, it should have all of the requisite bits.\nWe’ll load them into a GeoDataFrame object, which, as we will see, looks a lot like a pandas dataframe - but has a special column, geometry, which is what is used whenever we invoke the “plot” command.\nLet’s load the data - the geometries of London wards:\n\ndata_path = \"./data/wk9/london_wards.shp\"\n\n# londonWards = gp.GeoDataFrame.from_file(data_path)\n\nlondonWards = gp.read_file(data_path)\n\nlondonWards.head()"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#projection",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#projection",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Projection",
    "text": "Projection\nWe could spend a long time working through the merits of different projection methods; many of these are available in geopandas, through the PySAL library - more documentation is available there if you want to work with different projections.\nMore fundamentally, a Co-ordinate Reference System (CRS) refers to how we identify points on the globe; the world isn’t a perfect sphere so dealing with that is the job of the CRS. The WGS84 datum (https://en.wikipedia.org/wiki/World_Geodetic_System) which is sometimes called EPSG 4326 (epsg:4326).\nWe can find out the CRS of the geometry:\n\nlondonWards.crs\n\nIf we look at the geometry column, we see that they are latlon co-ordinates:\n\nlondonWards['geometry'].head()\n\nThis is all well and good, but we need to project the latlon data onto a plane to plot. We’ll create objects to keep track of these CRS frames. original_crs is the latlon co-ordinates; target_crs is a mercator (‘merc’) projection; and the third line, to_crs projects the geometry so we can plot it.\nExtension: You can find more detailed documentation about projection types and parameters at the PROJ.4 site which underpins pyproj/geopandas, and here: http://www.remotesensing.org/geotiff/proj_list/; in this case, it won’t make much difference which projection you use, because of the small scale.\n\nlondonWards.crs = 'epsg:4326'\ntarget_crs = {'datum':'WGS84', 'no_defs':True, 'proj':'merc'}\nprojected_londonWards = londonWards.to_crs(crs=target_crs)\n\nThe ‘geometry’ column isn’t in a lon, lat CRS any longer, but physical distances from some spatial point:\n\nprojected_londonWards['geometry'].head()\n\nPlotting this is now a simple matter:\n\nprojected_londonWards.plot()\n\nOur colours look a bit plain and boring. Let’s upgrade our plot to something more informative.\n\nprojected_londonWards.plot(column='UNIT_ID', cmap='rainbow', scheme='quantiles')\n\nHave a play around with mapping differnt coloums using different colours and different colouring schemes\nScheme must be in the set: ['quantiles', 'fisher_jenks', 'equal_interval']\nIf you want to play with the colour schemes for the map then take a look here. https://matplotlib.org/examples/color/colormaps_reference.html\nPossible values are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, inferno, inferno_r, jet, jet_r, magma, magma_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, seismic, seismic_r, spectral, spectral_r, spring, spring_r, summer, summer_r, terrain, terrain_r, viridis, viridis_r, winter, winter_r\nWe can also control the alpha of the map as well if we want to.\n\nprojected_londonWards.plot(column='UNIT_ID', cmap='rainbow', scheme='quantiles', alpha = 0.5)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#warding-off-evil",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#warding-off-evil",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Warding off evil",
    "text": "Warding off evil\nThe electoral wards of London represent voting areas for local councils; let’s now compare their populations to see where the population densities are in the city.\nPopulation data is available from the ONS - they link the population of different ages to a ‘Ward Code’, which is a standard identified of that ward: http://www.ons.gov.uk/ons/publications/re-reference-tables.html?edition=tcm%3A77-301951\nLet’s read it into a standard Pandas dataframe:\n\ndata_path = \"./data/wk9/2011persons.csv\"\n\npersons = pd.read_csv(data_path, encoding = 'latin1')\npersons.head()\n\nWe need to make sure the data is in the right format - to remove commas and dashes, as we have done before. I’m just going to clean up the “All Ages” column for this example, but you could apply this to any (or all) numerical columns.\n\npersons.replace(',', '', regex=True, inplace=True)\npersons['All Ages'] = persons['All Ages'].replace('-', 'NaN', regex=True).astype('float')\npersons['All Ages'].head()"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#exercise",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#exercise",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nUsing the Ward Code, merge the persons geodataframe with the londonWards dataframe to create a new geodataframe called “geopeople”. You can do do this in pretty much exactly the same way you’ve done for merging ‘vanilla’ dataframes - refer to the examples from earlier in the term if your syntax is rusty.\n\n\n\nOne important caveat: to plot the map, you need to be invoking .plot() on a geodataframe - so you need to merge on the geodataframe, with the dataframe as the argument. You can check this with the type(geopeople) command:\n\ntype(geopeople)\n\nCompare to merging on persons, which will give you a “pandas.core.frame.DataFrame” object - not a geodataframe.\nLet’s now calculate the population density using the ‘HECTARES’ column:\n\ngeopeople['density']=geopeople['All Ages']/geopeople['HECTARES']\ngeopeople.head()\n\nNameError: ignored"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#portable-data",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#portable-data",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Portable data",
    "text": "Portable data\nAt this point, we’ll save the data into a .csv so it’s easy to use in other software. This will be useful.\n\ndata_path = \"./data/wk9/borough_density.csv\"\n\ngeopeople.to_csv(data_path)\n\nAnd let’s project this into a new geodataframe:\n\noriginal_crs = geopeople.crs\ntarget_crs = {'datum':'WGS84', 'no_defs':True, 'proj':'merc'}\nprojected_geopeople = geopeople.to_crs(crs=target_crs)\n\n\nprojected_geopeople.plot(column='All Ages')\nplt.title('2011 population by Ward')\nplt.savefig('./data/wk9/Wards.png')"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#exercise-1",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#exercise-1",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Exercise",
    "text": "Exercise\nCreate a choropleth map of the population of each ward using projected_geopeople.plot() and save it as an image file. Use the optional arguments:\n\ncolumn, To specific the ‘All Ages’ column in the dataset to colour the map based on population, and\ncolormap, using the ‘Blues’ colormap to specify a white-blue colour range\n\n\n\n\n(It looks as if there aren’t population estimates for the smaller City of London Wards, so we’re missing them - if we wanted to show them, we would need a different dataset)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#between-the-bars",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#between-the-bars",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Between the Bars",
    "text": "Between the Bars\nWe have a nice map, but no colour scale to explain what the colours mean; unfortunately, this is a place where geopandas falls down and we have to do something intensely complex. The code you’ll see below was created by the inimitable Stephan Hugel to fill in the gaps (more for the superkeen here: sensitivecities.com/so-youd-like-to-make-a-map-using-python-EN.html#.VlSfMcq2-K7)\n\nimport numpy as np\nimport matplotlib\n\n\n\n# Convenience functions for working with colour ramps and bars\ndef colorbar_index(ncolors, cmap, labels=None, **kwargs):\n    \"\"\"\n    This is a convenience function to stop you making off-by-one errors\n    Takes a standard colour ramp, and discretizes it,\n    then draws a colour bar with correctly aligned labels\n    \"\"\"\n    cmap = cmap_discretize(cmap, ncolors)\n    mappable = plt.cm.ScalarMappable(cmap=cmap)\n    mappable.set_array([])\n    mappable.set_clim(-0.5, ncolors+0.5)\n    colorbar = matplotlib.pyplot.colorbar(mappable, **kwargs)\n    colorbar.set_ticks(np.linspace(0, ncolors, ncolors))\n    colorbar.set_ticklabels(range(ncolors))\n    if labels:\n        colorbar.set_ticklabels(labels)\n    return colorbar\n\ndef cmap_discretize(cmap, N):\n    \"\"\"\n    Return a discrete colormap from the continuous colormap cmap.\n\n        cmap: colormap instance, eg. cm.jet. \n        N: number of colors.\n\n    Example\n        x = resize(arange(100), (5,100))\n        djet = cmap_discretize(cm.jet, 5)\n        imshow(x, cmap=djet)\n\n    \"\"\"\n    if type(cmap) == str:\n        cmap = get_cmap(cmap)\n    colors_i = np.concatenate((np.linspace(0, 1., N), (0., 0., 0., 0.)))\n    colors_rgba = cmap(colors_i)\n    indices = np.linspace(0, 1., N + 1)\n    cdict = {}\n    for ki, key in enumerate(('red', 'green', 'blue')):\n        cdict[key] = [(indices[i], colors_rgba[i - 1, ki], colors_rgba[i, ki]) for i in range(N+1)]\n    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\" % N, cdict, 1024)\n\n\nfrom pysal.viz.mapclassify import Quantiles"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#thems-the-breaks",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#thems-the-breaks",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Them’s the breaks",
    "text": "Them’s the breaks\nThere are various ways to split the data, the default of which is quantiles - to order the data and then split the N data values we have evenly into q chunks, each of which has N/q data points. Consider the median - this splits the dataset into two chunks of size N/2; quartiles split the data into four chunks of size N/4; quantiles split the data into q chunks with N/q points. We will specify 5 chunks using the k=5 argument.\n(The documentation is here, if you’re interested in finding out more: https://github.com/pysal/pysal/blob/master/pysal/contrib/viz/mapping.py)\n\nbreaks = Quantiles(geopeople['density'].values, k=5)\nprint(breaks)\n\n\nprint(breaks.bins)\n\nWe’ll create a set of labels; again, this is a bit beyond what we’ve done before so don’t worry if it appears a bit mysterious.\n\nbar_labels = ['<=%i'% b for b in breaks.bins]\nprint(bar_labels)\n\n\nprojected_geopeople.plot(column='density', cmap='Greens', scheme='quantiles', k=5)\nplt.title('2011 population by Ward')\n\ncmap = plt.get_cmap('Greens')\ncolorbar_index(ncolors=5, cmap=cmap, shrink=0.5, labels=bar_labels)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#working-with-point-data-and-choropleths",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#working-with-point-data-and-choropleths",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Working with Point Data and Choropleths",
    "text": "Working with Point Data and Choropleths\nLet’s now go back to our twitter data and plot it on the same axes…\n\ndata_path = \"./data/wk9/tweet_data.csv\"\n\ntweets = pd.read_csv(data_path, parse_dates = [1], infer_datetime_format = True, encoding = 'latin1')\ntweets.head()\n\nWe will convert these points to a geometry object - previously, the geometry column stored data associated with the polygons for the wards - we’ll build geometries representing points and then convert the dataframe to a geodataframe. Again, the code below is a bit opaque - it’s a recipe for creating these geometries.\n\nfrom shapely.geometry import Point\ntweets['geometry'] = tweets.apply(lambda x: Point(x['Lon'], x['Lat']), axis=1)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#project-points-into-the-same-basis",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#project-points-into-the-same-basis",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Project points into the same basis",
    "text": "Project points into the same basis\nIn the next steps, we will 1) convert the dataframe to a geodataframe. 2) sets the CRS (co-ordinate reference system, remember) to the original CRS that we got from the wards data, and 3) projects into the same projection (using mercator, as before)\n\ngeotweets = gp.GeoDataFrame(tweets)\ngeotweets.crs = original_crs\ngeotweets.to_crs(crs=target_crs, inplace=True)\n\n\ngeotweets.head()\n\n\ngeotweets.plot()\n\n\nax = projected_geopeople.plot(column='density', cmap='Greens', scheme='quantiles', k=5)\nplt.title('2011 population by Ward')\n\ncmap = plt.get_cmap('Greens')\ncolorbar_index(ncolors=5, cmap=cmap, shrink=0.5, labels=bar_labels)\n\ngeotweets.plot(ax=ax)"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#extension",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#extension",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Extension:",
    "text": "Extension:\nWe can make these points a bit bigger by using a ‘buffer’ - this creates a geometry of circles rather than points:\n\ngeotweets['points'] = geotweets['geometry']\ngeotweets['geometry'] = gp.GeoSeries(tweets['points']).buffer(200)\ngeotweets.head()\n\nAnd we’ll add in a dummy variable we can use to make them all the same colour. Again, a bit clunky but means when we put them on a colormap (the object which converts a value to a color), it will give the same colour for each circle.\n\ngeotweets['dummy']=1\n\nNow we can plot this in the same projection, putting geopeople (which has the borough boundaries) on the same axes as the point twitter data.\n\nax = projected_geopeople.plot(column='density', cmap='Greens', scheme='quantiles', k=5)\nplt.title('2011 population by Ward')\n\ncmap = plt.get_cmap('Greens')\ncolorbar_index(ncolors=5, cmap=cmap, shrink=0.5, labels=bar_labels)\n\ngeotweets.plot(column='dummy', cmap='RdBu', ax=ax)\n\nThis has been a bit fiddly, but we have a tweet map with a choropleth of underlying residential population densities. You could look at evening-time tweets only and then we’re starting to get somewhere. But this is just an example, so you can now use this as a recipe for your own maps. Or…"
  },
  {
    "objectID": "notebooks/W10. Choropleth Maps and Polygons.html#alternatives-to-python",
    "href": "notebooks/W10. Choropleth Maps and Polygons.html#alternatives-to-python",
    "title": "Quantitative Methods 2: Data Science and Visualisation",
    "section": "Alternatives to Python",
    "text": "Alternatives to Python\nIt’s worth experimenting with alternatives if you intend to create more complex maps. For example, Carto is an option for producing choropleths. Here is a brief tutorial of how to map population density based on the csv we created (borough_density.csv) that has the unprojected borough polygons and their associated population densities. It’s still sensible to do data linking and processing here in python, as it makes the final stages fairly straightforward - note that Carto works with latlon (unprojected) data rather than the projected/OSGB data.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"fofRRwZjiyg\")"
  }
]